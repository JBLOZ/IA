\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish,es-tabla]{babel}
\usepackage[margin=2cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage[hypertexnames=false]{hyperref}
\usepackage{float}

\graphicspath{{images/}}

\title{\textbf{Práctica 1: Aprendizaje Supervisado}\\
\large Comparativa de SVM, Árboles y Ensembles sobre el dataset de Obesidad}

\author{
    \textbf{Jordi Blasco Lozano}\\
    \small DNI: 74527208D\\
    \small Universidad de Alicante - Escuela Politécnica Superior\\
    \small Aprendizaje Avanzado - Curso 2025/2026\\
    \small Email: \texttt{jbl42@alu.ua.es}
}

\date{}

\newcommand{\tbl}[3]{
\begin{table}[H]
\centering
\caption{#2}
\label{#3}
\scriptsize
\input{tables/#1}
\end{table}
}

\begin{document}

\maketitle

\begin{abstract}
\textit{En esta práctica se evalúan algoritmos de aprendizaje supervisado sobre el mismo problema de clasificación multiclase de la Práctica 0 (niveles de obesidad). Se mantiene exactamente el mismo dataset y el mismo tratamiento de datos para garantizar comparabilidad: eliminación IQR en Weight/Height, transformación Box-Cox en Age, filtrado IQR posterior, codificación binaria + one-hot, split estratificado 80/20 y filtro Z-score en train. Sobre esta base se comparan SVM, árboles de decisión, Random Forest/Extra Trees y Gradient Boosting/AdaBoost mediante validación cruzada 5-fold, métricas weighted y análisis de hiperparámetros. El mejor rendimiento global se obtiene con Gradient Boosting optimizado, alcanzando \textbf{Accuracy = 0.9716} y \textbf{F1-weighted = 0.9718} en test.}
\end{abstract}

\section{Introducción}
Se reutiliza el dataset \textit{Estimation of Obesity Levels Based on Eating Habits and Physical Condition} (UCI) para comparar directamente con la Práctica 0. El objetivo aquí no es reexplicar el preprocesado, sino analizar de forma compacta el comportamiento de varios modelos supervisados y sus hiperparámetros sobre la misma base experimental.

\section{Configuración Experimental}
\subsection{Pipeline de datos heredado de Práctica 0}
Se aplicó el mismo flujo de limpieza y codificación para mantener paridad experimental. El control de paridad obtenido se resume en la Tabla~\ref{tab:checks}.

\tbl{00_preprocessing_checks.tex}{Check de paridad del preprocesamiento respecto a Práctica 0}{tab:checks}

Interpretación breve: los conteos y la transformación de Age coinciden con la práctica anterior (\(2111 \to 2109 \to 2107\), \(\lambda\approx -1.6415\), split \(1685/422\), sin eliminaciones adicionales por Z-score en train).

\subsection{EDA resumido}
Se mantuvieron las mismas visualizaciones clave (distribuciones, boxplots, correlación, scatter, balance de clases, PCA), con texto reducido. Como recordatorio visual:

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{class_distribution.png}
\caption{Distribución de clases tras el tratamiento de datos (mismo pipeline de P0).}
\label{fig:class_dist}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.78\textwidth]{correlacion_matrix.png}
\caption{Matriz de correlación de variables numéricas.}
\label{fig:corr}
\end{figure}

\section{Resultados}
\subsection{Parte 1: Support Vector Machines}
Se evaluaron kernels por defecto, optimización de RBF y polinomial, comparación de implementaciones lineales y análisis de vectores de soporte.

\tbl{10_svm_kernels_default.tex}{SVM con kernels por defecto}{tab:svm_default}
\tbl{12_svm_rbf_grid_representative.tex}{Grid representativo de SVM RBF (\(C\), \(\gamma\))}{tab:svm_rbf_rep}
\tbl{13_svm_poly_grid_top12.tex}{Top-12 configuraciones de SVM polinomial}{tab:svm_poly_top}
\tbl{14_svm_linear_implementations.tex}{Comparación de implementaciones lineales}{tab:svm_linear_impl}
\tbl{15_svm_support_vectors_analysis.tex}{Análisis de vectores de soporte por configuración}{tab:svm_sv_analysis}
\tbl{16_svm_part1_summary.tex}{Resumen final Parte 1 (mejor configuración por familia)}{tab:svm_summary}

Hallazgos: el mejor SVM fue \textbf{lineal} (test \(0.9716\), F1 \(0.9715\)) y además con menor porcentaje de SV (16.08\%). En RBF, la mejor zona fue \(C\) alto y \(\gamma\) bajo-moderado.

\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{confusion_matrix_svm.png}
\caption{Matriz de confusión del mejor modelo SVM de la parte 1.}
\label{fig:cm_svm}
\end{figure}

\subsection{Parte 2: Árboles de Decisión}
Se analizó árbol baseline, profundidad, criterio, poda previa (grid), poda posterior (\(ccp\_alpha\)), importancia y visualización.

\tbl{20_tree_baseline.tex}{Árbol sin restricciones (baseline)}{tab:tree_base}
\tbl{21_tree_depth_sweep.tex}{Barrido de profundidad máxima}{tab:tree_depth}
\tbl{22_tree_criteria.tex}{Comparación de criterios de división}{tab:tree_criteria}
\tbl{24_tree_prepruning_grid_top12.tex}{Top-12 configuraciones de poda previa (GridSearch)}{tab:tree_pre}
\tbl{26_tree_ccp_representative.tex}{Valores representativos de poda posterior (\(ccp\_alpha\))}{tab:tree_ccp}
\tbl{27_tree_feature_importance_top10.tex}{Top-10 importancias de features (árbol)}{tab:tree_imp}
\tbl{28_tree_part2_summary.tex}{Resumen final Parte 2}{tab:tree_summary}

Hallazgos: aunque la mejor CV aparece en poda previa/posterior, el baseline sin restricciones mantiene test competitivo (0.9289). En criterio, \texttt{entropy/log\_loss} superan a \texttt{gini} en esta práctica.

\begin{figure}[H]
\centering
\includegraphics[width=0.92\textwidth]{decision_tree.png}
\caption{Visualización del árbol (profundidad limitada para legibilidad).}
\label{fig:tree_plot}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{confusion_matrix_tree.png}
\caption{Matriz de confusión del mejor árbol en la parte 2.}
\label{fig:cm_tree}
\end{figure}

\subsection{Parte 3: Random Forest}
Se evaluó RF por defecto, barridos de \(n\_estimators\) y \(max\_features\), grid search, importancias (Gini+Permutation) y Extra Trees.

\tbl{30_rf_default.tex}{Random Forest por defecto}{tab:rf_default}
\tbl{31_rf_estimators_sweep.tex}{Efecto del número de estimadores}{tab:rf_estimators}
\tbl{32_rf_max_features_sweep.tex}{Efecto de \texttt{max\_features}}{tab:rf_maxfeat}
\tbl{34_rf_grid_top12.tex}{Top-12 configuraciones de RF (GridSearch)}{tab:rf_grid}
\tbl{35_rf_feature_importance_top10.tex}{Top-10 importancias de features en RF}{tab:rf_imp}
\tbl{37_rf_part3_summary.tex}{Resumen final Parte 3}{tab:rf_summary}

Hallazgos: la mejor configuración RF alcanza \(0.9550\) en test y mejora al RF por defecto. El ranking de importancia confirma el peso dominante de \texttt{Weight} y \texttt{Height}.

\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{confusion_matrix_rf.png}
\caption{Matriz de confusión del mejor modelo de la parte 3.}
\label{fig:cm_rf}
\end{figure}

\subsection{Parte 4: Gradient Boosting y AdaBoost}
Se analizó GB por defecto, barridos \((n\_estimators, learning\_rate)\), profundidad, grid search y AdaBoost con stump.

\tbl{40_gb_default.tex}{Gradient Boosting por defecto}{tab:gb_default}
\tbl{41_gb_estimators_lr_sweep.tex}{Efecto conjunto de \(n\_estimators\) y \(learning\_rate\)}{tab:gb_lr}
\tbl{42_gb_depth_sweep.tex}{Efecto de \texttt{max\_depth} en GB}{tab:gb_depth}
\tbl{44_gb_grid_top12.tex}{Top-12 configuraciones de GB (GridSearch)}{tab:gb_grid}
\tbl{45_adaboost_sweep.tex}{Barrido de AdaBoost}{tab:ada_sweep}
\tbl{46_boosting_part4_summary.tex}{Resumen final Parte 4}{tab:boost_summary}

Hallazgos: Gradient Boosting optimizado fue el mejor de esta parte (test \(0.9716\), F1 \(0.9718\)). En este dataset, AdaBoost con stump quedó muy por debajo del resto.

\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{confusion_matrix_boosting.png}
\caption{Matriz de confusión del mejor modelo de la parte 4.}
\label{fig:cm_boost}
\end{figure}

\section{Comparativa Global}
La comparación final de mejores modelos por familia se muestra en la Tabla~\ref{tab:global}. El mejor modelo global fue \textbf{Gradient Boosting}.

\tbl{50_global_comparison.tex}{Comparativa final global de modelos}{tab:global}

\begin{figure}[H]
\centering
\includegraphics[width=0.72\textwidth]{confusion_matrix_final.png}
\caption{Matriz de confusión del mejor modelo global (Gradient Boosting).}
\label{fig:cm_final}
\end{figure}

\section{Conclusiones}
\begin{itemize}
    \item Mantener exactamente el pipeline de P0 permitió comparar modelos en igualdad de condiciones.
    \item En SVM, el kernel lineal fue superior al resto en este problema tras optimización de \(C\).
    \item En árboles, la poda mejoró la estabilidad en CV, pero el baseline siguió siendo competitivo en test.
    \item Random Forest mejoró claramente a un árbol individual y confirmó la relevancia de \texttt{Weight}/\texttt{Height}.
    \item El mejor resultado global fue Gradient Boosting optimizado (Accuracy 0.9716, F1-weighted 0.9718).
\end{itemize}

\end{document}
