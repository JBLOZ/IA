{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6f1bd2",
   "metadata": {},
   "source": [
    "# Práctica 1 — Dataset 1: Incidentes policiales de San Francisco (Bronze ➜ Silver)\n",
    "\n",
    "**Asignatura:** PMDV (Curso 2025-2026)  \n",
    "**Objetivo:** Leer el CSV de incidentes desde la capa **bronze** (MinIO), limpiar/transformar con **PySpark DataFrames** y persistir en **silver** en formato **Parquet**.\n",
    "\n",
    "Este notebook sigue *paso a paso* el enunciado de la práctica (Dataset 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf630d",
   "metadata": {},
   "source": [
    "## Requisitos previos (manual)\n",
    "\n",
    "1. Descarga desde **Moodle**: `sf_police_incidents_2018_to_2025_1208.zip`\n",
    "2. Descomprime el ZIP y localiza el fichero:\n",
    "   - `Police_Department_Incident_Reports__2018_to_Present_20251208.csv`\n",
    "3. Sube **el CSV** al bucket **bronze** de MinIO, dentro de la carpeta/prefijo:\n",
    "\n",
    "   - `sf_police_incidents/`\n",
    "\n",
    "4. Verifica que puedes leer desde Spark con `s3a://bronze/...` y escribir en `s3a://silver/...`.\n",
    "\n",
    "> Importante: la lectura debe hacerse **a nivel de carpeta**, no apuntando al CSV individual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b79f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0) Imports + SparkSession\n",
    "# ============================================================\n",
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "import os, re, unicodedata\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Recomendación: fija la zona horaria de la sesión (evita sorpresas con timestamps)\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0.1) Configuración de acceso a MinIO (S3A)\n",
    "# ============================================================\n",
    "# Si tu entorno ya viene configurado (por ejemplo via spark-defaults.conf),\n",
    "# esta celda puede ser redundante. Aun así, es útil como \"recordatorio\" y para local.\n",
    "#\n",
    "# Ajusta estos valores si tu despliegue usa otros nombres/puertos.\n",
    "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\", \"http://minio:9000\")\n",
    "MINIO_ACCESS_KEY = os.getenv(\"MINIO_ACCESS_KEY\", \"minioadmin\")\n",
    "MINIO_SECRET_KEY = os.getenv(\"MINIO_SECRET_KEY\", \"minioadmin\")\n",
    "MINIO_USE_SSL = os.getenv(\"MINIO_USE_SSL\", \"false\").lower() == \"true\"\n",
    "\n",
    "hconf = spark._jsc.hadoopConfiguration()\n",
    "hconf.set(\"fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "hconf.set(\"fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "hconf.set(\"fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "hconf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hconf.set(\"fs.s3a.connection.ssl.enabled\", \"true\" if MINIO_USE_SSL else \"false\")\n",
    "hconf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "hconf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "print(\"MINIO_ENDPOINT =\", MINIO_ENDPOINT)\n",
    "print(\"MINIO_ACCESS_KEY =\", MINIO_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476fb079",
   "metadata": {},
   "source": [
    "## 1) Lectura desde Bronze (sin inferir esquema)\n",
    "\n",
    "- Se lee el dataset desde la carpeta en **bronze**.\n",
    "- No se infiere el esquema (`inferSchema = false`), por lo que inicialmente las columnas vendrán como `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f8e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRONZE_PATH = \"s3a://bronze/sf_police_incidents/\"\n",
    "\n",
    "df_raw = (\n",
    "    spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", \"false\")\n",
    "        .csv(BRONZE_PATH)\n",
    ")\n",
    "\n",
    "df_raw.printSchema()\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09e036b",
   "metadata": {},
   "source": [
    "## 2) Selección de columnas requeridas\n",
    "\n",
    "Seleccionamos **solo** las columnas pedidas en el enunciado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = [\n",
    "    \"Incident ID\",\n",
    "    \"Incident Number\",\n",
    "    \"Row ID\",\n",
    "    \"Incident Datetime\",\n",
    "    \"Report Datetime\",\n",
    "    \"Incident Code\",\n",
    "    \"Incident Category\",\n",
    "    \"Incident Subcategory\",\n",
    "    \"Incident Description\",\n",
    "    \"Report Type Code\",\n",
    "    \"Resolution\",\n",
    "    \"Police District\",\n",
    "    \"Analysis Neighborhood\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "]\n",
    "\n",
    "df_selected = df_raw.select(*required_cols)\n",
    "\n",
    "df_selected.printSchema()\n",
    "df_selected.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c95908",
   "metadata": {},
   "source": [
    "## 3) Enfoque alternativo: eliminar columnas que no necesitas (sin usar `select`)\n",
    "\n",
    "En el paso 2 hemos usado `select(...)` para quedarnos con las columnas necesarias.\n",
    "\n",
    "Una alternativa equivalente es **eliminar** las columnas sobrantes con `drop(...)`.\n",
    "Esto es útil cuando tienes claro qué columnas **no** quieres (por ejemplo, una lista larga)\n",
    "o cuando quieres mantener el orden original del dataset, pero sin campos innecesarios.\n",
    "\n",
    "Documentación oficial (Spark 4.0.1):  \n",
    "- `pyspark.sql.DataFrame.drop`: https://spark.apache.org/docs/4.0.1/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.drop.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e8eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [c for c in df_raw.columns if c not in required_cols]\n",
    "\n",
    "df_alt = df_raw.drop(*cols_to_drop)\n",
    "\n",
    "print(\"Mismas columnas (como conjunto):\", set(df_alt.columns) == set(df_selected.columns))\n",
    "print(\"Nº columnas df_selected:\", len(df_selected.columns))\n",
    "print(\"Nº columnas df_alt     :\", len(df_alt.columns))\n",
    "\n",
    "df_alt.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50e7574",
   "metadata": {},
   "source": [
    "## 4) Renombrado a `snake_case`\n",
    "\n",
    "Reglas:\n",
    "- minúsculas\n",
    "- palabras separadas por `_`\n",
    "- sin espacios ni caracteres especiales\n",
    "\n",
    "Ejemplo: `Incident Datetime` → `incident_datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(name: str) -> str:\n",
    "    # Normaliza acentos y caracteres raros\n",
    "    name = unicodedata.normalize(\"NFKD\", name).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    # Sustituye no-alfanumérico por espacio\n",
    "    name = re.sub(r\"[^0-9a-zA-Z]+\", \" \", name)\n",
    "    # Separa camel case (por si acaso) y colapsa espacios\n",
    "    name = re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1 \\2\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip().lower()\n",
    "    # Une con underscore\n",
    "    return name.replace(\" \", \"_\")\n",
    "\n",
    "snake_cols = [to_snake_case(c) for c in df_selected.columns]\n",
    "df = df_selected.toDF(*snake_cols)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68803deb",
   "metadata": {},
   "source": [
    "## 5) Convertir `incident_datetime` y `report_datetime` a `timestamp`\n",
    "\n",
    "Como el CSV puede venir con formatos distintos (dependiendo de la exportación),\n",
    "usamos un parseo “robusto” intentando varios patrones y quedándonos con el primero que funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp(col):\n",
    "    # Intenta varios formatos comunes (ISO 8601, con/sin milisegundos, con/sin TZ)\n",
    "    return F.coalesce(\n",
    "        F.to_timestamp(col, \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"),\n",
    "        F.to_timestamp(col, \"yyyy-MM-dd'T'HH:mm:ss.SSS\"),\n",
    "        F.to_timestamp(col, \"yyyy-MM-dd'T'HH:mm:ssX\"),\n",
    "        F.to_timestamp(col, \"yyyy-MM-dd'T'HH:mm:ss\"),\n",
    "        F.to_timestamp(col, \"yyyy-MM-dd HH:mm:ss\"),\n",
    "        F.to_timestamp(col)  # fallback: formato por defecto\n",
    "    )\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\"incident_datetime\", parse_timestamp(F.col(\"incident_datetime\")))\n",
    "      .withColumn(\"report_datetime\", parse_timestamp(F.col(\"report_datetime\")))\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d976baf6",
   "metadata": {},
   "source": [
    "## 6) Convertir el resto de columnas numéricas a tipos adecuados\n",
    "\n",
    "Convertimos identificadores y coordenadas a tipos numéricos soportados por Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8556fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.withColumn(\"incident_id\", F.col(\"incident_id\").cast(T.LongType()))\n",
    "      .withColumn(\"incident_number\", F.col(\"incident_number\").cast(T.LongType()))\n",
    "      .withColumn(\"row_id\", F.col(\"row_id\").cast(T.LongType()))\n",
    "      .withColumn(\"incident_code\", F.col(\"incident_code\").cast(T.IntegerType()))\n",
    "      .withColumn(\"latitude\", F.col(\"latitude\").cast(T.DoubleType()))\n",
    "      .withColumn(\"longitude\", F.col(\"longitude\").cast(T.DoubleType()))\n",
    ")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285de12",
   "metadata": {},
   "source": [
    "## 7) Crear `reporting_delay_minutes`\n",
    "\n",
    "Diferencia en minutos entre `report_datetime` e `incident_datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3643a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    \"reporting_delay_minutes\",\n",
    "    ((F.col(\"report_datetime\").cast(\"long\") - F.col(\"incident_datetime\").cast(\"long\")) / 60).cast(\"int\")\n",
    ")\n",
    "\n",
    "df.select(\"incident_datetime\", \"report_datetime\", \"reporting_delay_minutes\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6109b758",
   "metadata": {},
   "source": [
    "## 8) Crear `delay_bucket` (categoría por rango de minutos)\n",
    "\n",
    "Rangos requeridos (valores **string**):\n",
    "- `<0`\n",
    "- `0_10`\n",
    "- `10_60`\n",
    "- `60_1440`\n",
    "- `>1440`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cc2711",
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = F.col(\"reporting_delay_minutes\")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"delay_bucket\",\n",
    "    F.when(delay.isNull(), F.lit(None).cast(\"string\"))\n",
    "     .when(delay < 0, F.lit(\"<0\"))\n",
    "     .when((delay >= 0) & (delay < 10), F.lit(\"0_10\"))\n",
    "     .when((delay >= 10) & (delay < 60), F.lit(\"10_60\"))\n",
    "     .when((delay >= 60) & (delay <= 1440), F.lit(\"60_1440\"))\n",
    "     .otherwise(F.lit(\">1440\"))\n",
    ")\n",
    "\n",
    "df.groupBy(\"delay_bucket\").count().orderBy(\"delay_bucket\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfbb769",
   "metadata": {},
   "source": [
    "## 9) Escritura en Silver (Parquet)\n",
    "\n",
    "Persistimos el DataFrame final en:\n",
    "\n",
    "- `silver/sf_police_incidents/`\n",
    "\n",
    "usando formato **Parquet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88631394",
   "metadata": {},
   "outputs": [],
   "source": [
    "SILVER_PATH = \"s3a://silver/sf_police_incidents/\"\n",
    "\n",
    "(\n",
    "    df.write\n",
    "      .mode(\"overwrite\")\n",
    "      .parquet(SILVER_PATH)\n",
    ")\n",
    "\n",
    "print(\"Escritura completada en:\", SILVER_PATH)\n",
    "\n",
    "# (Opcional) Lectura de verificación\n",
    "df_check = spark.read.parquet(SILVER_PATH)\n",
    "print(\"Filas en silver:\", df_check.count())\n",
    "df_check.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29850f",
   "metadata": {},
   "source": [
    "## 10) Preguntas analíticas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041684d9",
   "metadata": {},
   "source": [
    "### i) ¿Cuántas filas tienen valores nulos en `latitude` o `longitude`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d580e",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_latlon_rows = df.filter(F.col(\"latitude\").isNull() | F.col(\"longitude\").isNull()).count()\n",
    "print(\"Filas con latitude o longitude nulos:\", null_latlon_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a370f9",
   "metadata": {},
   "source": [
    "**Respuesta (completa tras ejecutar):**  \n",
    "- **Resultado:** `TODO: sustituye por el número impreso arriba`  \n",
    "- **Comentario:** estas filas no tienen geolocalización completa, por lo que (según el análisis posterior) pueden requerir filtrado,\n",
    "  imputación o tratarse como “sin localización” en visualizaciones/mapas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd5892",
   "metadata": {},
   "source": [
    "### ii) ¿Cuántos `incident_id` aparecen más de una vez en el dataset?\n",
    "\n",
    "Ten en cuenta que un mismo incidente puede aparecer en varias filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_incident_ids = (\n",
    "    df.groupBy(\"incident_id\")\n",
    "      .count()\n",
    "      .filter(F.col(\"count\") > 1)\n",
    "      .count()\n",
    ")\n",
    "\n",
    "print(\"Número de incident_id que aparecen más de una vez:\", duplicated_incident_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d3b75",
   "metadata": {},
   "source": [
    "**Respuesta (completa tras ejecutar):**  \n",
    "- **Resultado:** `TODO: sustituye por el número impreso arriba`  \n",
    "- **Comentario:** esto indica cuántos IDs de incidente tienen varias filas (por ejemplo, por desgloses o múltiples registros asociados).\n",
    "  En análisis posteriores, conviene decidir si el “evento” es `incident_id` (agregando) o si se trabaja a nivel de fila."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f8c6e",
   "metadata": {},
   "source": [
    "### iii) ¿Cuántos incidentes tienen un retraso entre 10 y 60 minutos en su procesamiento?\n",
    "\n",
    "Usa exclusivamente la columna `delay_bucket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opción A: contar filas cuyo bucket sea 10_60\n",
    "rows_10_60 = df.filter(F.col(\"delay_bucket\") == \"10_60\").count()\n",
    "\n",
    "# Opción B (recomendable si quieres \"incidentes\" únicos): contar incident_id distintos en ese bucket\n",
    "incidents_10_60 = (\n",
    "    df.filter(F.col(\"delay_bucket\") == \"10_60\")\n",
    "      .select(\"incident_id\")\n",
    "      .distinct()\n",
    "      .count()\n",
    ")\n",
    "\n",
    "print(\"Filas con delay_bucket == '10_60':\", rows_10_60)\n",
    "print(\"Incident_id distintos con delay_bucket == '10_60':\", incidents_10_60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f495a1",
   "metadata": {},
   "source": [
    "**Respuesta (completa tras ejecutar):**  \n",
    "- **Resultado (según interpretación):**\n",
    "  - Filas: `TODO`\n",
    "  - Incidentes únicos (`incident_id` distintos): `TODO`\n",
    "- **Comentario:** el enunciado habla de “incidentes”, así que suele tener sentido reportar **IDs distintos**.\n",
    "  En cualquier caso, el filtrado se hace usando únicamente `delay_bucket`, tal y como se pide."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
