{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Práctica 2 — Reinforcement Learning (Taxi-v3)\n\n**Autor:** Jordi Blasco Lozano  \n**Asignatura:** Agentes Inteligentes\n\n### Índice\n1. Resumen\n2. Configuración y utilidades\n3. Ejercicio 1: política aleatoria (Random Walk)\n4. Ejercicio 2: SARSA\n5. Comparativa final y conclusiones\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Resumen\n\nEn esta práctica implemento y comparo dos enfoques sobre **Taxi-v3**:\n\n- Un agente sin aprendizaje (acciones aleatorias), como baseline.\n- Un agente entrenado con **SARSA** (método on-policy de control por valor).\n\nPara ambos casos recojo las métricas pedidas en el enunciado:\n\n- **Recompensa acumulada** por episodio.\n- **Número de timesteps** por episodio.\n- **Número de penalizaciones** (acciones ilegales de recoger/dejar pasajero).\n\nTambién incluyo funciones para generar GIFs de episodios representativos.\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Configuración y utilidades"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Si necesitas instalar dependencias, descomenta y ejecuta:\n# !pip install \"gymnasium[toy-text]\" numpy matplotlib moviepy IPython pandas\n\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Tuple\n\nimport gymnasium as gym\nimport moviepy as mp\nfrom IPython.display import Image\n\nplt.style.use(\"seaborn-v0_8\")\nnp.set_printoptions(precision=4, suppress=True)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def save_gif(frames, filename, fps=10):\n    \"\"\"Crea y serializa un GIF a partir de una lista de frames RGB.\"\"\"\n    clip = mp.ImageSequenceClip(frames, fps=fps)\n    clip.write_gif(filename, fps=fps)\n\n\ndef summarize_metrics(metrics: Dict[str, List[float]]) -> Dict[str, Tuple[float, float]]:\n    \"\"\"Devuelve media y desviación típica para cada métrica.\"\"\"\n    return {\n        key: (float(np.mean(values)), float(np.std(values)))\n        for key, values in metrics.items()\n    }\n\n\ndef print_summary(title: str, summary: Dict[str, Tuple[float, float]]):\n    print(f\"\\n{title}\")\n    print(\"-\" * len(title))\n    for metric, (mean, std) in summary.items():\n        print(f\"{metric:>20}: media = {mean:8.3f} | std = {std:8.3f}\")\n\n\ndef plot_metrics(metrics: Dict[str, List[float]], title_prefix: str):\n    fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n    names = [\"rewards\", \"timesteps\", \"penalties\"]\n    labels = [\"Recompensa acumulada\", \"Timesteps\", \"Penalizaciones\"]\n\n    for ax, name, label in zip(axes, names, labels):\n        ax.plot(metrics[name], alpha=0.9)\n        ax.set_title(f\"{title_prefix} — {label}\")\n        ax.set_xlabel(\"Episodio\")\n        ax.set_ylabel(label)\n\n    plt.tight_layout()\n    plt.show()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Ejercicio 1: política aleatoria (Random Walk)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "En esta sección evalúo durante **100 episodios** una política completamente aleatoria.\n\nAdemás, implemento una variante opcional con **action masking** para muestrear solo acciones legales cuando el entorno lo permite."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "@dataclass\nclass EpisodeResult:\n    reward: float\n    timesteps: int\n    penalties: int\n\n\ndef run_random_episode(env, use_action_mask=False, capture_frames=False, max_steps=200):\n    state, info = env.reset()\n\n    total_reward = 0\n    penalties = 0\n    frames = []\n\n    for t in range(1, max_steps + 1):\n        if use_action_mask and \"action_mask\" in info:\n            valid_actions = np.flatnonzero(info[\"action_mask\"])\n            action = int(np.random.choice(valid_actions))\n        else:\n            action = env.action_space.sample()\n\n        next_state, reward, terminated, truncated, info = env.step(action)\n\n        if reward == -10:\n            penalties += 1\n\n        total_reward += reward\n        state = next_state\n\n        if capture_frames:\n            frame = env.render()\n            if frame is not None:\n                frames.append(frame)\n\n        if terminated or truncated:\n            return EpisodeResult(total_reward, t, penalties), frames\n\n    return EpisodeResult(total_reward, max_steps, penalties), frames\n\n\ndef evaluate_random_policy(n_episodes=100, use_action_mask=False, seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n\n    metrics = {\"rewards\": [], \"timesteps\": [], \"penalties\": []}\n\n    for _ in range(n_episodes):\n        result, _ = run_random_episode(env, use_action_mask=use_action_mask)\n        metrics[\"rewards\"].append(result.reward)\n        metrics[\"timesteps\"].append(result.timesteps)\n        metrics[\"penalties\"].append(result.penalties)\n\n    gif_result, gif_frames = run_random_episode(\n        env,\n        use_action_mask=use_action_mask,\n        capture_frames=True,\n    )\n\n    env.close()\n    return metrics, gif_result, gif_frames\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "random_metrics, random_gif_result, random_frames = evaluate_random_policy(\n    n_episodes=100,\n    use_action_mask=False,\n    seed=123,\n)\n\nrandom_summary = summarize_metrics(random_metrics)\nprint_summary(\"Política aleatoria (sin action mask)\", random_summary)\nplot_metrics(random_metrics, \"Random policy\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "random_gif_path = \"random_policy.gif\"\nsave_gif(random_frames, random_gif_path, fps=8)\nImage(filename=random_gif_path)\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "masked_metrics, masked_gif_result, masked_frames = evaluate_random_policy(\n    n_episodes=100,\n    use_action_mask=True,\n    seed=123,\n)\n\nmasked_summary = summarize_metrics(masked_metrics)\nprint_summary(\"Política aleatoria (con action mask)\", masked_summary)\nplot_metrics(masked_metrics, \"Random policy + action mask\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Ejercicio 2: SARSA"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Implemento SARSA con estrategia **$\\epsilon$-greedy** para entrenar una tabla $Q$ de tamaño `(500, 6)`.\n\nHiperparámetros base (ajustables):\n\n- `alpha = 0.1` (learning rate)\n- `gamma = 0.99` (discount factor)\n- `epsilon_start = 1.0`, `epsilon_min = 0.05`, `epsilon_decay = 0.995`\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def epsilon_greedy_action(q_table, state, epsilon, action_space_n):\n    if np.random.random() < epsilon:\n        return np.random.randint(action_space_n)\n    return int(np.argmax(q_table[state]))\n\n\ndef train_sarsa(\n    n_episodes=5000,\n    alpha=0.1,\n    gamma=0.99,\n    epsilon_start=1.0,\n    epsilon_min=0.05,\n    epsilon_decay=0.995,\n    max_steps=200,\n    seed=42,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    env = gym.make(\"Taxi-v3\")\n    n_states = env.observation_space.n\n    n_actions = env.action_space.n\n\n    q_table = np.zeros((n_states, n_actions), dtype=np.float32)\n\n    metrics = {\"rewards\": [], \"timesteps\": [], \"penalties\": []}\n    epsilon = epsilon_start\n\n    for _ in range(n_episodes):\n        state, _ = env.reset()\n        action = epsilon_greedy_action(q_table, state, epsilon, n_actions)\n\n        total_reward = 0\n        penalties = 0\n\n        for t in range(1, max_steps + 1):\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            next_action = epsilon_greedy_action(q_table, next_state, epsilon, n_actions)\n\n            td_target = reward + gamma * q_table[next_state, next_action]\n            td_error = td_target - q_table[state, action]\n            q_table[state, action] += alpha * td_error\n\n            if reward == -10:\n                penalties += 1\n\n            total_reward += reward\n            state, action = next_state, next_action\n\n            if terminated or truncated:\n                break\n\n        metrics[\"rewards\"].append(total_reward)\n        metrics[\"timesteps\"].append(t)\n        metrics[\"penalties\"].append(penalties)\n\n        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n\n    env.close()\n    return q_table, metrics\n\n\ndef evaluate_greedy_policy(q_table, n_episodes=100, max_steps=200, seed=123, capture_gif=True):\n    random.seed(seed)\n    np.random.seed(seed)\n\n    env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n    metrics = {\"rewards\": [], \"timesteps\": [], \"penalties\": []}\n    gif_frames = []\n\n    for ep in range(n_episodes):\n        state, _ = env.reset()\n        total_reward = 0\n        penalties = 0\n        local_frames = []\n\n        for t in range(1, max_steps + 1):\n            action = int(np.argmax(q_table[state]))\n            next_state, reward, terminated, truncated, _ = env.step(action)\n\n            if reward == -10:\n                penalties += 1\n\n            total_reward += reward\n            state = next_state\n\n            if capture_gif and ep == 0:\n                frame = env.render()\n                if frame is not None:\n                    local_frames.append(frame)\n\n            if terminated or truncated:\n                break\n\n        metrics[\"rewards\"].append(total_reward)\n        metrics[\"timesteps\"].append(t)\n        metrics[\"penalties\"].append(penalties)\n\n        if capture_gif and ep == 0:\n            gif_frames = local_frames\n\n    env.close()\n    return metrics, gif_frames\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "q_table, sarsa_train_metrics = train_sarsa(\n    n_episodes=5000,\n    alpha=0.1,\n    gamma=0.99,\n    epsilon_start=1.0,\n    epsilon_min=0.05,\n    epsilon_decay=0.995,\n    max_steps=200,\n    seed=123,\n)\n\ntrain_summary = summarize_metrics(sarsa_train_metrics)\nprint_summary(\"SARSA - entrenamiento\", train_summary)\nplot_metrics(sarsa_train_metrics, \"SARSA training\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "sarsa_eval_metrics, learned_frames = evaluate_greedy_policy(\n    q_table,\n    n_episodes=100,\n    max_steps=200,\n    seed=123,\n    capture_gif=True,\n)\n\nsarsa_eval_summary = summarize_metrics(sarsa_eval_metrics)\nprint_summary(\"SARSA - evaluación (política greedy)\", sarsa_eval_summary)\nplot_metrics(sarsa_eval_metrics, \"SARSA evaluation\")\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "learned_gif_path = \"learned_policy.gif\"\nsave_gif(learned_frames, learned_gif_path, fps=8)\nImage(filename=learned_gif_path)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Comparativa final y conclusiones"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import pandas as pd\n\ncomparison = pd.DataFrame(\n    {\n        \"Random\": [\n            random_summary[\"rewards\"][0],\n            random_summary[\"timesteps\"][0],\n            random_summary[\"penalties\"][0],\n        ],\n        \"Random + Mask\": [\n            masked_summary[\"rewards\"][0],\n            masked_summary[\"timesteps\"][0],\n            masked_summary[\"penalties\"][0],\n        ],\n        \"SARSA (eval)\": [\n            sarsa_eval_summary[\"rewards\"][0],\n            sarsa_eval_summary[\"timesteps\"][0],\n            sarsa_eval_summary[\"penalties\"][0],\n        ],\n    },\n    index=[\"Reward medio\", \"Timesteps medios\", \"Penalizaciones medias\"],\n)\n\ncomparison\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Discusión\n\n- La política aleatoria presenta recompensas bajas, episodios largos y muchas penalizaciones.\n- El action masking suele reducir penalizaciones, pero no aporta planificación global.\n- SARSA aprende una política claramente superior: mejora recompensa media, reduce timesteps y minimiza acciones ilegales.\n\nEn conjunto, el aprendizaje basado en valor convierte el comportamiento del taxi de un paseo errático a una navegación dirigida por objetivo.\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}