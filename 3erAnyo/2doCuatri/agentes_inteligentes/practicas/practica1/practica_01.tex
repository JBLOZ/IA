%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[T1]{fontenc}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% Eliminar el texto de "Proceedings of ICML..." de la primera página
\makeatletter
\renewcommand{\ICML@appearing}{}
\makeatother

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Custom table spacing
\usepackage{tabla}

% For list customization
\usepackage{enumitem}
\setlist[itemize]{noitemsep, topsep=1pt, leftmargin=*}
\setlist[enumerate]{noitemsep, topsep=0pt, leftmargin=*}

% Pie de página personalizado en la última página
\usepackage{lastpage}
\makeatletter
\fancypagestyle{lastpagestyle}{%
  \fancyhf{}
  \fancyhead{}
  \chead{}
  \cfoot{\thepage\\[0.3cm]\small\hypertarget{author1}{}\textsuperscript{1}Jordi Blasco Lozano -- DNI: 74527208D -- jordiblloz@gmail.com}
  \renewcommand{\headrulewidth}{1pt}
}
\AtEndDocument{\thispagestyle{lastpagestyle}}
\makeatother

% The \icmltitle you define below is probably too long as a header.
\icmltitlerunning{Análisis Comparativo de Graph Neural Networks: GCN vs MLP}

\begin{document}

\twocolumn[
\icmltitle{Análisis Comparativo de Graph Neural Networks: GCN vs MLP \\ en Clasificación de Nodos}

\begin{icmlauthorlist}
\icmlauthor{Jordi Blasco Lozano\hyperlink{author1}{\textsuperscript{1}}}{}
\end{icmlauthorlist}

\icmlkeywords{GNN, GCN, Machine Learning, Deep Learning, Graphs}

\vskip 0.3in
]

\begin{abstract}
Práctica 1 Graph Neural Networks (GNNs). Se ha investigado el comportamiento de las GNNs frente a modelos tradicionales (MLP) utilizando tanto un dataset sintético diseñado específicamente para aislar el valor de la estructura del grafo, como datasets de referencia (Cora y Citeseer). Los experimentos demuestran que las GCN superan dramáticamente a los MLP (99.9\% vs 32.2\% de accuracy) en escenarios donde las características de los nodos son ruidosas pero la homofilia es alta, confirmando que el mecanismo de message passing es crucial para recuperar la señal subyacente.
\end{abstract}

\section{Introducción}
\label{introduction}

En esta práctica, nos centramos en la tarea de \textbf{clasificación de nodos semi-supervisada}, donde solo una pequeña fracción de los nodos tiene etiquetas conocidas y debemos predecir el resto. El objetivo central es evidenciar las limitaciones de los modelos que ignoran la estructura relacional, como el Perceptrón Multicapa (MLP), y contrastarlos con modelos mejor diseñados para operar sobre grafos, específicamente las Graph Convolutional Networks (GCN).

La hipótesis fundamental que validaremos es que el mecanismo de \textit{message passing} de las GCN permite mitigar el ruido en las características individuales mediante la agregación de información del vecindario, un proceso que es imposible para un MLP que asume independencia entre muestras.

\section{Creación del Dataset Sintético}
\label{dataset}

Para el análisis, no basta con usar datasets estándar donde no controlamos las propiedades de los datos. Se ha diseñado y generado el (\textit{Custom Dataset}) con el objetivo de demostrar las diferencias fundamentales entre los modelos.

\textbf{Generación de la Estructura:}
Se utilizó el modelo \textbf{Stochastic Block Model} para generar un grafo con las siguientes características:
\begin{itemize}
    \item \textbf{Nodos:} 2000 nodos divididos en 4 comunidades del mismo tamaño.
    \item \textbf{Homofilia Estructural:} Se definió una probabilidad de conexión intra-clase $p_{in}=0.02$ y una inter-clase $p_{out}=0.001$. Esto asegura que, topológicamente, los nodos de la misma clase estén densamente conectados, creando comunidades bien definidas.
\end{itemize}

\textbf{Generación de Características:}
Se generaron los vectores de características $\mathbf{x} \in \mathbb{R}^{32}$. Se utilizaron los mismos valores de señal y ruido del enunciado de la practica. Las características se generan utilizando la siguiente función.
$$ \mathbf{x}_i = \alpha \cdot \mathbf{centroide}_{y_i} + \mathcal{N}(0, \sigma^2) $$
Donde la señal de la clase ($\alpha$) es débil en comparación con la magnitud del ruido gaussiano ($\sigma^2$ alto).

Si las características fueran perfectas, un MLP obtendría casi 100\% de acierto y la estructura del grafo sería irrelevante. Al introducir mucho ruido, forzamos al modelo a depender de la estructura: un modelo debe tener en cuenta a los vecinos para desambiguar la clase del nodo. Esto simula escenarios reales donde la información local es imperfecta.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{images/image.png}
\end{figure}

\begin{figure}[h]
    \caption{Visualización de la estructura del grafo sintético generado (SBM). Se observan claramente las 4 comunidades densamente conectadas internamente.}
    \label{fig:graph_structure}
\end{figure}

Tal como se aprecia en la \cref{fig:graph_structure}, la topología resultante es muy informativa. A pesar del ruido en los nodos individuales, un nodo está casi siempre conectado a nodos de su misma clase, lo cual es la premisa que explota la GCN.

\section{Modelos Implementados}
\label{models}

\subsection{Multi-Layer Perceptron (MLP)}
Se implementó como una red feedforward con:
\begin{itemize}
    \item Capa de entrada: Proyección lineal de dimensiones de entrada a ocultas.
    \item Función de activación: ReLU.
    \item Dropout: $p=0.5$ para regularización.
    \item Capa de salida: Proyección lineal al número de clases.
\end{itemize}
Matemáticamente, para un nodo $i$, la salida depende exclusivamente de $\mathbf{x}_i$. Este modelo es ciego a la matriz de adyacencia $\mathbf{A}$.

\subsection{Graph Convolutional Network (GCN)}
Cada capa realiza la operación:
$$ \mathbf{H}^{(l+1)} = \sigma(\tilde{\mathbf{D}}^{-1/2}\tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-1/2} \mathbf{H}^{(l)} \mathbf{W}^{(l)}) $$
Donde $\tilde{\mathbf{A}}$ es la matriz de adyacencia con self-loops. Esta operación realiza una suma ponderada de las características del propio nodo y las de sus vecinos, seguido de una transformación lineal y una no-linealidad. En nuestro contexto de ruido gaussiano, este promedio local reduce la varianza del ruido en un factor proporcional al grado del nodo (por la Ley de los Grandes Números), permitiendo que emerja la señal del centroide de la clase.

\section{Resultados y Análisis}
\label{results}

Se realizaron experimentos utilizando validación cruzada con 10 ejecuciones independientes para garantizar unos resultados robustos.

\subsection{Rendimiento en Dataset Custom}

Los resultados en el dataset sintético son contundentes y confirman nuestra hipótesis de diseño.

\begin{table}[h]
\caption{Resultados en Dataset Custom (10 runs)}
\label{tab:results_custom}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
\textbf{Modelo} & \textbf{Acc Media} & \textbf{Std Dev} & \textbf{Gap} \\
\midrule
MLP & 0.3223 & $\pm$ 0.0175 & - \\
GCN & \textbf{0.9998} & $\pm$ 0.0007 & +67.7\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Análisis del colapso del MLP (0.322):}
Dado que hay 4 clases, un clasificador aleatorio obtendría un 0.25 (25\%). El MLP con 0.32 apenas supera el azar. Esto indica que el ruido introducido en las características es tan alto que destruye casi toda la información discriminativa a nivel individual. El MLP no puede ver más allá del ruido de cada feature aislada.

\textbf{Análisis del éxito de la GCN (0.999):}
La GCN alcanza la perfección. A pesar de que cada nodo individualmente es ruidoso e inclasificable, el colectivo (la comunidad) contiene la información perfecta. La topología SBM que creamos actúa como un mecanismo de corrección de errores extremadamente potente. La GCN ha aprendido a suavizar las características usando la estructura, recuperando la señal original casi intacta.

\subsection{Validación en Benchmarks Reales}

Para verificar que estas conclusiones no son un artefacto de nuestro dataset sintético, evaluamos los modelos en redes de citación reales: \textbf{Cora} y \textbf{Citeseer}.

\begin{table}[h]
\caption{Resultados en Benchmarks Reales}
\label{tab:benchmarks}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{\textbf{Cora}} & \multicolumn{2}{c}{\textbf{Citeseer}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Modelo} & \textbf{Acc} & \textbf{Gap} & \textbf{Acc} & \textbf{Gap} \\
\midrule
MLP & 0.560 & - & 0.549 & - \\
GCN & \textbf{0.807} & +24.7\% & \textbf{0.690} & +14.1\% \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\textbf{Disminución del Gap:}
Aunque la GCN sigue siendo muy superior (+24.7\% en Cora), el MLP ya no falla estrepitosamente (0.56 vs 0.32). Esto se debe a que, en Cora, las características son palabras (Bag-of-Words). La presencia de palabras técnicas específicas (ej: "genoma") es por sí misma un predictor fuerte de la clase, independientemente de las citas. Sin embargo, la GCN en Cora logra un 80\% al combinar esta información semántica con la información de citación (quién cita a quién). También hay que recalcar que la GCN, pese a tener un rendimiento decente en ambos datasets, no llega a la perfección de nuestro dataset sintético. Y esto se debe a que los grafos reales presentan una homofilia imperfecta y una estructura topológica más compleja. A diferencia del SBM, que impone comunidades matemáticamente segregadas, las redes de citación reales contienen enlaces transversales y fronteras difusas entre temas que limitan la capacidad de separación perfecta mediante la propagación de mensajes. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/final_comparison.png}
    \caption{Comparativa final de Accuracy en los tres datasets evaluados.}
    \label{fig:final_comparison}
\end{figure}

La \cref{fig:final_comparison} resume estos hallazgos, mostrando que aunque la magnitud de la mejora varía según la calidad de las features (máxima en Custom, moderada en Cora/Citeseer), la GCN es sistemáticamente superior.

\subsection{Impacto de Hiperparámetros y Entrenamiento}

\textbf{Curvas de Aprendizaje:}
Observamos que la GCN converge mucho más rápido y a pérdidas mucho menores. El MLP se estanca prematuramente en un valle de error alto, incapaz de optimizar más allá debido a la inconsistencia de las etiquetas respecto a las características ruidosas.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/custom_base_training_curves.png}
    \caption{Curvas de entrenamiento (pérdida y accuracy). La GCN (naranja) alcanza rápidamente la convergencia, mientras el MLP (roja) no logra aprender.}
    \label{fig:training_curves}
\end{figure}

En la \cref{fig:training_curves} se hace evidente la disparidad en la dinámica de aprendizaje. Mientras la loss de la GCN cae a cero, la del MLP permanece alta, indicando que el modelo no encuentra patrones fiables en las características aisladas. Podemos observar como vemos un comportamiento extraño en el plot de training, vemos como pese a saber que MLP es un modelo malo para nuestro dataset, la curva roja sube en esta gráfica a mas de 0.8 mientras que en la gráfica de validación se mantiene en todo momento al rededor del 0.3, esto se debe a que el modelo está experimentando un \textbf{sobreajuste (overfitting) masivo}. El MLP, al no poder apoyarse en la estructura para cancelar el ruido, opta por memorizar el ruido específico de las muestras de entrenamiento para reducir la pérdida. Sin embargo, dado que este ruido es aleatorio y no contiene información generalizable, la exactitud en validación permanece estancada, revelando que el modelo no ha aprendido ninguna característica discriminativa real. 

\textbf{Análisis de Hiperparámetros:}
Para validar la robustez de los modelos, he extendido la búsqueda de hiperparámetros a los tres datasets (Custom, Cora, Citeseer).

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp_hidden_dims.png}
    \caption{Impacto de las dimensiones ocultas en los tres datasets (Accuracy).}
    \label{fig:hidden_dims}
\end{figure}

\textbf{Dimensiones Ocultas (\cref{fig:hidden_dims}):}
Al analizar las dimensiones [16, 32, 64, 128], observamos que el rendimiento se mantiene prácticamente idéntico indistintamente del tamaño de la capa oculta. El modelo con 16 dimensiones obtiene resultados indistinguibles del de 128. Esto sugiere que el problema de clasificación no requiere una gran capacidad de representación; las características latentes necesarias son lo suficientemente compactas como para ser capturadas incluso por la configuración más pequeña, indicando que el cuello de botella no está en la anchura de la red sino en la propia estructura de los datos.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp_learning_rate.png}
    \caption{Sensibilidad al Learning Rate.}
    \label{fig:learning_rate}
\end{figure}

\textbf{Learning Rate (\cref{fig:learning_rate}):}
El LR de 0.01 se confirma como el punto óptimo. La caída drástica observada en 0.0001 (especialmente en Cora con GCN bajando al 77.5\%) se debe a una convergencia insuficiente. Con pasos de actualización tan pequeños y un número de épocas fijo (200), el optimizador no logra recorrer la superficie de error hasta el mínimo global, quedándose atascado en zonas subóptimas. En Cora, cuya superficie de optimización es más irregular que en el dataset sintético, este efecto se acentúa porque el modelo necesita actualizaciones más agresivas para escapar de los mínimos locales iniciales.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp_dropout.png}
    \caption{Efecto del Dropout.}
    \label{fig:dropout}
\end{figure}

\textbf{Regularización - Dropout (\cref{fig:dropout}):}
Al probar tasas de Dropout desde 0.0 hasta 0.9, los resultados muestran una invarianza casi total. No observamos ninguna mejora significativa ni deterioro en el accuracy final al modificar este hiperparámetro. Tanto en el dataset sintético como en los reales, la GCN mantiene su rendimiento intacto independientemente de la probabilidad de desconexión. Esto refuerza la idea de que la regularización principal en las GCN viene dada por la propia convolución (suavizado de vecinos) y no por mecanismos explícitos como el Dropout en esta configuración.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp_weight_decay.png}
    \caption{Impacto del Weight Decay (L2).}
    \label{fig:weight_decay}
\end{figure}

\textbf{Regularización - Weight Decay (\cref{fig:weight_decay}):}
El Weight Decay aplica una penalización L2 a los pesos ($\lambda ||w||^2$) para mantenerlos pequeños y evitar funciones de decisión excesivamente complejas. Al igual que con el dropout, observamos que no importa demasiado el valor que pongamos, los resultados se mantienen planos. Esto sugiere que las GCN, por su naturaleza de promediado local (filtro paso-bajo), son intrínsecamente regulares. La propia operación de convolución ya suaviza la señal y previene cambios bruscos, actuando como una regularización implícita potente, lo que hace que la regularización explícita (L2) tenga un impacto marginal comparado con modelos no estructurados.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{images/exp_optimizer.png}
    \caption{Comparativa de Optimizadores.}
    \label{fig:optimizer}
\end{figure}

\textbf{Optimizadores (\cref{fig:optimizer}):}
Para entender los resultados, primero definamos brevemente los optimizadores a utilizar:
\begin{itemize}
    \item \textbf{SGD:} Descenso de gradiente estocástico básico, actualiza pesos en dirección opuesta al gradiente.
    \item \textbf{Adam:} Combina inercia (promedio de gradientes pasados para suavizar dirección) con adaptación (normalización por magnitudes históricas), ajustando tasa y estabilidad de forma autónoma.
\end{itemize}

Los resultados muestran como SGD fracasa estrepitosamente en los datasets reales (Cora 30\%, Citeseer 26\%), comportándose casi como el azar. Sin mecanismos adaptativos ni momentum, SGD es incapaz de navegar la geometría compleja y dispersa de los grafos, quedándose atascado inmediatamente. Por el contrario, Adam logra converger eficazmente a valores altos (88\% y 76\%), demostrando que para entrenar GNNs de forma fiable, el uso de optimizadores adaptativos que gestionen la escala de los gradientes es una muy buena práctica.


\section{Espacio Latente (t-SNE)}

La visualización de los embeddings aprendidos mediante t-SNE proporciona la evidencia más intuitiva de la capacidad de la GCN para estructurar el espacio latente.

\begin{figure}[ht]
    \centering
    \subfigure[Embeddings MLP]{
        \includegraphics[width=0.9\columnwidth]{images/mlp_embeddings_tsne.png}
        \label{fig:mlp_tsne}
    }
    \hfill
    \subfigure[Embeddings GCN]{
        \includegraphics[width=0.9\columnwidth]{images/gcn_embeddings_tsne.png}
        \label{fig:gcn_tsne}
    }
    \caption{Visualización t-SNE del espacio latente aprendido por ambos modelos en el dataset sintético.}
    \label{fig:tsne}
\end{figure}

\begin{itemize}
    \item \textbf{MLP (\cref{fig:mlp_tsne}):} Muestra una nube dispersa donde las clases están severamente mezcladas (puntos de distintos colores superpuestos). Dado que el modelo solo ve características ruidosas, no logra proyectar los datos a un espacio donde sean linealmente separables.
    \item \textbf{GCN (\cref{fig:gcn_tsne}):} Muestra clústeres extremadamente compactos y bien separados, con márgenes amplios entre clases. La estructura del grafo ha actuado como una fuerza de cohesión, obligando a los nodos conectados a tener representaciones vectoriales similares, limpiando efectivamente el ruido.
\end{itemize}

\section{Conclusiones}
\label{conclusions}

Esta práctica ha servido para deconstruir el funcionamiento de las Graph Neural Networks.
\begin{enumerate}
    \item Hemos demostrado que la información estructural es un recurso tan valioso como la información de características. Ignorarla, como hace el MLP, es desperdiciar la mitad de los datos disponibles.
    \item El dataset sintético probó que la GCN es extraordinariamente robusta al ruido de entrada (0.99 de acierto vs 0.32), comportándose como un potente filtro de denoising estructural.
    \item En datos reales, aunque las características sean informativas, la GCN proporciona una mejora crítica (+14-25\%) al modelar el contexto relacional.
\end{enumerate}

Como conclusión final, las GCN no son solo otra arquitectura, sino un cambio de paradigma: pasamos de aprender de entidades aisladas a aprender de sistemas interconectados.


\end{document}
