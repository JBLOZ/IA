{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdaI3aj-JVun"
   },
   "source": [
    "# CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# CONFIG\n",
    "FAST_RUN = True  # pon False si quieres ejecutar TODO (descargas/entreno completo)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjGc-lerIJos"
   },
   "source": [
    "In this notebook, we are going to implement a state-of-the-art convolutional neural network. You can use PyTorch, Keras, or TensorFlow for this purpose. However, you should provide a summary of the created network at the end. You have to implement an architecture between AlexNet and VGG19 and another architecture between Resnet50 and Inception v1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "form",
    "id": "wYW9y-ttKsYs"
   },
   "source": [
    "### Ejemplo de summary (solo ilustrativo)\n",
    "\n",
    "Este bloque estaba como *code cell* y provocaba `SyntaxError`. Lo dejamos como ejemplo en texto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qaocy40LqE3"
   },
   "source": [
    "## [VGG19 Architecture ](https://arxiv.org/pdf/1409.1556.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITaDW_QoH8Y9"
   },
   "source": [
    "[![VGG19](https://drive.google.com/uc?export=view&id=1RVNnaJexXo1DmQGdLN-JIs6NwJDR9Ikb)](https://drive.google.com/uc?export=view&id=1RVNnaJexXo1DmQGdLN-JIs6NwJDR9Ikb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBUBNSHVMmrc"
   },
   "source": [
    "## [AlexNet Architecture](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYX7BxQEIJAH"
   },
   "source": [
    "[![AlexNet](https://drive.google.com/uc?export=view&id=1oQyk0IPqSsc5P2fAZXMplfcrEsHI_MUR)](https://drive.google.com/uc?export=view&id=1oQyk0IPqSsc5P2fAZXMplfcrEsHI_MUR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeN9-dAIspDe"
   },
   "source": [
    "## [Resnet50 Architecture](https://arxiv.org/abs/1512.03385)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYZq1GqJtG3t"
   },
   "source": [
    "[![Resnet50](https://drive.google.com/uc?export=view&id=1bMm3lrl8hQjVz1epH6WwEC24RFtMMUFo)](https://drive.google.com/uc?export=view&id=1bMm3lrl8hQjVz1epH6WwEC24RFtMMUFo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sMpLLUKsozk"
   },
   "source": [
    "## [Inception v3 Architecture](https://arxiv.org/abs/1512.00567v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVzpxLDNtQ_6"
   },
   "source": [
    "[![Inception v1](https://drive.google.com/uc?export=view&id=1n-_IIBxsXviJRxH9Be6L2at6cnzzzuAq)](https://drive.google.com/uc?export=view&id=1n-_IIBxsXviJRxH9Be6L2at6cnzzzuAq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea6OPN-SyjbI"
   },
   "source": [
    "## **First Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWnl3qFXyyl_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementación manual de VGG19 siguiendo la arquitectura del paper\n",
    "    'Very Deep Convolutional Networks for Large-Scale Image Recognition'\n",
    "    (Simonyan & Zisserman, 2014).\n",
    "\n",
    "    Atributos:\n",
    "        features (nn.Sequential): 5 bloques convolucionales con filtros 3x3.\n",
    "        avgpool (nn.AdaptiveAvgPool2d): Pooling adaptativo para salida fija de 7x7.\n",
    "        classifier (nn.Sequential): 3 capas fully-connected para clasificación.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(VGG, self).__init__()\n",
    "                                                                                            # Input: [batch, 3, 224, 224]\n",
    "        self.features = nn.Sequential(\n",
    "            # Bloque 1: 2 capas conv de 64 filtros 3x3\n",
    "            nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),            # [batch, 64, 224, 224]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),           # [batch, 64, 224, 224]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),  # [batch, 64, 112, 112]\n",
    "\n",
    "            # Bloque 2: 2 capas conv de 128 filtros 3x3\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),          # [batch, 128, 112, 112]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 128, 112, 112]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),  # [batch, 128, 56, 56]\n",
    "\n",
    "            # Bloque 3: 4 capas conv de 256 filtros 3x3 (VGG19 tiene 4, VGG16 tiene 3)\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 256, 56, 56]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 256, 56, 56]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 256, 56, 56]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 256, 56, 56]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),  # [batch, 256, 28, 28]\n",
    "\n",
    "            # Bloque 4: 4 capas conv de 512 filtros 3x3\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 512, 28, 28]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 512, 28, 28]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 512, 28, 28]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 512, 28, 28]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),  # [batch, 512, 14, 14]\n",
    "\n",
    "            # Bloque 5: 4 capas conv de 512 filtros 3x3\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 512, 14, 14]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 512, 14, 14]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 512, 14, 14]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),         # [batch, 512, 14, 14]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),  # [batch, 512, 7, 7]\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(7, 7))                            # [batch, 512, 7, 7]\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=512 * 7 * 7, out_features=4096, bias=True),               # [batch, 4096]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=4096, bias=True),                       # [batch, 4096]\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(in_features=4096, out_features=num_classes, bias=True),                # [batch, num_classes]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instanciar y mostrar resumen de la arquitectura\n",
    "my_vgg19 = VGG(num_classes=1000)\n",
    "print(my_vgg19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFQ4FvzpyydC"
   },
   "source": [
    "## **Second Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QLqHGnWYyyTq"
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Bloque Bottleneck para ResNet.\n",
    "    Arquitectura: conv1x1 -> BN -> ReLU -> conv3x3 -> BN -> ReLU -> conv1x1 -> BN + skip -> ReLU\n",
    "    El factor de expansión es 4 (el tercer conv1x1 multiplica los canales por 4).\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        # Reducción de dimensionalidad: conv 1x1\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        # Procesado espacial: conv 3x3 (aquí se aplica el stride)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=(3, 3), stride=(stride, stride), padding=(1, 1), bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        # Expansión de canales: conv 1x1\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        # Conexión residual (skip connection)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementación manual de ResNet.\n",
    "    Basada en 'Deep Residual Learning for Image Recognition' (He et al., 2015).\n",
    "\n",
    "    Para ResNet50 se usa layers=[3, 4, 6, 3] con bloques Bottleneck.\n",
    "    Las conexiones residuales permiten entrenar redes muy profundas sin\n",
    "    degradación del gradiente.\n",
    "    \"\"\"\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "                                                                                                    # Input: [batch, 3, 224, 224]\n",
    "        # Stem: conv7x7 + BN + ReLU + MaxPool\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)  # [batch, 64, 112, 112]\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)   # [batch, 64, 56, 56]\n",
    "\n",
    "        # 4 stages de bloques residuales\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])                                    # [batch, 256, 56, 56]\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)                         # [batch, 512, 28, 28]\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)                         # [batch, 1024, 14, 14]\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)                         # [batch, 2048, 7, 7]\n",
    "\n",
    "        # Cabecera de clasificación\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))                                 # [batch, 2048, 1, 1]\n",
    "        self.fc = nn.Linear(in_features=512 * block.expansion, out_features=num_classes, bias=True)  # [batch, num_classes]\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        \"\"\"Construye un stage de bloques residuales.\"\"\"\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=(1, 1), stride=(stride, stride), bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        # Bloques residuales\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # Clasificación\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ResNet50: 4 stages con [3, 4, 6, 3] bloques Bottleneck\n",
    "my_resnet50 = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=1000)\n",
    "print(my_resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2T16QubMmBj"
   },
   "source": [
    "## Auxiliary code to check the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzIm0DmmMlkc"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "alexnet = models.alexnet(weights=None)\n",
    "vgg19 = models.vgg19(weights=None)\n",
    "resnet50 = models.resnet50(weights=None)\n",
    "inceptionv3 = models.inception_v3(weights=None, aux_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9om57-MVNQ1O"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import sys\n",
    "\n",
    "def capture_model_print(model):\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = buffer = io.StringIO()\n",
    "    print(model)\n",
    "    sys.stdout = old_stdout\n",
    "    return buffer.getvalue()\n",
    "\n",
    "# Verificar VGG19: comparar implementación manual vs torchvision\n",
    "custom_vgg = capture_model_print(my_vgg19)\n",
    "pytorch_vgg = capture_model_print(vgg19)\n",
    "\n",
    "if custom_vgg == pytorch_vgg:\n",
    "    print(\"VGG19: Los modelos tienen estructuras idénticas. ✓\")\n",
    "else:\n",
    "    print(\"VGG19: Las estructuras difieren. ✗\")\n",
    "\n",
    "# Verificar ResNet50: comparar implementación manual vs torchvision\n",
    "custom_res = capture_model_print(my_resnet50)\n",
    "pytorch_res = capture_model_print(resnet50)\n",
    "\n",
    "if custom_res == pytorch_res:\n",
    "    print(\"ResNet50: Los modelos tienen estructuras idénticas. ✓\")\n",
    "else:\n",
    "    print(\"ResNet50: Las estructuras difieren. ✗\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17PcaX60KK_M"
   },
   "source": [
    "# Dataset and Training\n",
    "\n",
    "Now we are going to train one of the implemented networks. For this, we will use the CIFAR-10 dataset. This dataset is composed of 10 classes but we will only use 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybfwgmM9OkCe"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWS_aY-aO6td"
   },
   "outputs": [],
   "source": [
    "#@title Dataset\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize to [-1, 1] range\n",
    "])\n",
    "\n",
    "def filter_indices_with_limits(dataset, class_sample_limits):\n",
    "    indices = []\n",
    "    class_counts = {cls: 0 for cls in class_sample_limits.keys()}\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        class_name = dataset.classes[label]\n",
    "        if class_name in class_sample_limits:\n",
    "            if class_counts[class_name] < class_sample_limits[class_name]:\n",
    "                indices.append(i)\n",
    "                class_counts[class_name] += 1\n",
    "            if all(class_counts[cls] >= limit for cls, limit in class_sample_limits.items()):\n",
    "                break\n",
    "    return indices\n",
    "\n",
    "\n",
    "trainset = datasets.CIFAR10(root='CIFAR10_data/', train=True, download=True, transform=transform)\n",
    "testset = datasets.CIFAR10(root='CIFAR10_data/', train=False, download=True, transform=transform)\n",
    "\n",
    "train_limits = {'airplane': 500, 'automobile': 500, 'bird': 50}\n",
    "test_limits = {'airplane': 100, 'automobile': 100, 'bird': 100}\n",
    "\n",
    "train_indices = filter_indices_with_limits(trainset, train_limits)\n",
    "trainset_filtered = Subset(trainset, train_indices)\n",
    "\n",
    "test_indices = filter_indices_with_limits(testset, test_limits)\n",
    "testset_filtered = Subset(testset, test_indices)\n",
    "\n",
    "trainloader = DataLoader(trainset_filtered, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset_filtered, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxaRzby2Jybi"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "As you can see, we have a dataset that is not balanced. The `bird` class only has 50 samples, while `airplane` and `automobile` have 500. In this section, you will need to implement a data augmentation technique to solve this problem in our dataset.\n",
    "\n",
    "You can find examples of different techniques at the following link:\n",
    "\n",
    "1. [Getting started with transforms v2](https://pytorch.org/vision/stable/auto_examples/transforms/plot_transforms_getting_started.html#sphx-glr-auto-examples-transforms-plot-transforms-getting-started-py)\n",
    "2. [Transforming and augmenting images](https://pytorch.org/vision/stable/transforms.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1WX7BivEpDP"
   },
   "source": [
    "### **Apply Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hgsTNUU6miK"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "# Transformación con data augmentation agresiva para la clase minoritaria\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Crear dataset CIFAR10 con las transformaciones de augmentation\n",
    "trainset_augmented = datasets.CIFAR10(root='CIFAR10_data/', train=True, download=False, transform=augment_transform)\n",
    "\n",
    "# Obtener los primeros 50 índices de la clase bird de forma eficiente\n",
    "bird_class_idx = trainset_augmented.class_to_idx['bird']\n",
    "bird_base_indices = [i for i, t in enumerate(trainset_augmented.targets) if t == bird_class_idx][:50]\n",
    "\n",
    "# Repetir los índices 9 veces → 50 × 9 = 450 muestras augmentadas\n",
    "# Cada repetición genera variaciones distintas gracias a las transformaciones aleatorias\n",
    "augmented_bird_indices = bird_base_indices * 9\n",
    "bird_augmented_subset = Subset(trainset_augmented, augmented_bird_indices)\n",
    "\n",
    "# Dataset balanceado: original (500 airplane + 500 auto + 50 bird) + 450 bird augmentadas\n",
    "trainset_balanced = ConcatDataset([trainset_filtered, bird_augmented_subset])\n",
    "trainloader_balanced = DataLoader(trainset_balanced, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Dataset original (desbalanceado): {len(trainset_filtered)} muestras\")\n",
    "print(f\"  - airplane: 500, automobile: 500, bird: 50\")\n",
    "print(f\"Dataset balanceado: {len(trainset_balanced)} muestras\")\n",
    "print(f\"  - airplane: 500, automobile: 500, bird: 50 + 450 aug = 500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bo41_U9ORwr3"
   },
   "source": [
    "# Fine-Tunning\n",
    "\n",
    "Now we are going to perform fine-tuning on the two architectures that have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uk6UwM1PRwYp"
   },
   "outputs": [],
   "source": [
    "# Fine-tuning: Cargar pesos preentrenados de VGG19 en nuestra implementación manual\n",
    "model = VGG(num_classes=1000)\n",
    "model.load_state_dict(models.vgg19(weights=None).state_dict())\n",
    "\n",
    "# Modificar el clasificador para 3 clases\n",
    "num_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(num_features, 3)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"VGG19 con fine-tuning para 3 clases listo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "C7ZSxBkVSnvH"
   },
   "outputs": [],
   "source": [
    "#@title Loss Function and Optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAOK-2FeS4WO",
    "outputId": "b13ef94b-d999-4891-ddca-4f1f41fda627"
   },
   "outputs": [],
   "source": [
    "# FAST_RUN: reducimos a 1 epoch\n",
    "if FAST_RUN:\n",
    "    num_epochs = 1\n",
    "\n",
    "#@title Training\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HawWiKlfTWul",
    "outputId": "4690c71b-9d78-4a3e-c1e2-f05e83f86298"
   },
   "outputs": [],
   "source": [
    "#@title Test the model\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the model on the test images: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "YI-PS-LxTbPX",
    "outputId": "905b06e0-22a6-4923-8615-d6a7ed55f28d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Get a batch of test images and labels\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(images.to(device))\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Function to unnormalize and plot an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "\n",
    "classes = trainset.classes[0:3]\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "for idx in range(3):\n",
    "    ax = fig.add_subplot(1, 3, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(f\"True: {classes[labels[idx]]}\\nPredicted: {classes[predicted[idx]]}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUQUV-Jb8Nal"
   },
   "source": [
    "# **Questions**\n",
    "\n",
    "1. Describe the techniques used to balance the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLWZET2hK3ql"
   },
   "source": [
    "**Respuesta:**\n",
    "\n",
    "Para equilibrar el dataset se ha utilizado **data augmentation** (aumento de datos) sobre la clase minoritaria (`bird`, 50 muestras) para igualarla con las clases mayoritarias (`airplane` y `automobile`, 500 muestras cada una).\n",
    "\n",
    "Las técnicas de augmentation aplicadas son:\n",
    "\n",
    "* **RandomHorizontalFlip (p=0.5):** Voltea horizontalmente la imagen con un 50% de probabilidad. Es válido porque un pájaro volteado sigue siendo un pájaro.\n",
    "* **RandomRotation (20°):** Rota la imagen aleatoriamente hasta ±20 grados, generando variaciones en la orientación del objeto.\n",
    "* **ColorJitter (brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1):** Modifica aleatoriamente las propiedades cromáticas de la imagen, simulando distintas condiciones de iluminación y ambientación.\n",
    "* **RandomAffine (translate=0.1, scale=0.9-1.1):** Aplica traslaciones y escalados aleatorios, simulando diferentes posiciones y tamaños del objeto en el encuadre.\n",
    "\n",
    "Se han generado **450 muestras augmentadas adicionales** de `bird` repitiendo las 50 imágenes originales 9 veces con transformaciones aleatorias. Al ser estocásticas, cada repetición produce una imagen visualmente distinta, generando diversidad en el conjunto de entrenamiento. El resultado es un dataset balanceado de 1500 muestras totales (500 por clase)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4etzU0R9K4Dr"
   },
   "source": [
    "2. Train both networks with balanced and unbalanced datasets. How does it affect the predictions? Display a confusion matrix for each case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kECImJYFK4j0"
   },
   "outputs": [],
   "source": [
    "# FAST_RUN: reducimos a 1 epoch\n",
    "if FAST_RUN:\n",
    "    num_epochs = 1\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def train_and_evaluate(model, train_loader, test_loader, num_epochs = 1, lr=1e-5, model_name=\"Model\"):\n",
    "    \"\"\"Entrena un modelo y devuelve predicciones, targets y pérdidas por época.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(test_loader))\n",
    "\n",
    "        print(f\"[{model_name}] Epoch {epoch+1}/{num_epochs} - Train Loss: {epoch_loss:.4f} - Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "    # Evaluación final\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(labels.cpu().numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_targets), train_losses, val_losses\n",
    "\n",
    "\n",
    "def plot_confusion(y_true, y_pred, class_names, title):\n",
    "    \"\"\"Muestra la matriz de confusión.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "classes = ['airplane', 'automobile', 'bird']\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "# ==================== VGG19 - Desbalanceado ====================\n",
    "print(\"=\" * 60)\n",
    "print(\"VGG19 - Dataset DESBALANCEADO\")\n",
    "print(\"=\" * 60)\n",
    "vgg_unbal = VGG(num_classes=1000)\n",
    "vgg_unbal.load_state_dict(models.vgg19(weights=None).state_dict())\n",
    "vgg_unbal.classifier[6] = nn.Linear(4096, 3)\n",
    "\n",
    "preds_vgg_u, targets_vgg_u, tl_vgg_u, vl_vgg_u = train_and_evaluate(\n",
    "    vgg_unbal, trainloader, testloader, NUM_EPOCHS, model_name=\"VGG19-Desbalanceado\")\n",
    "plot_confusion(targets_vgg_u, preds_vgg_u, classes, \"VGG19 - Desbalanceado\")\n",
    "\n",
    "# ==================== VGG19 - Balanceado ====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VGG19 - Dataset BALANCEADO\")\n",
    "print(\"=\" * 60)\n",
    "vgg_bal = VGG(num_classes=1000)\n",
    "vgg_bal.load_state_dict(models.vgg19(weights=None).state_dict())\n",
    "vgg_bal.classifier[6] = nn.Linear(4096, 3)\n",
    "\n",
    "preds_vgg_b, targets_vgg_b, tl_vgg_b, vl_vgg_b = train_and_evaluate(\n",
    "    vgg_bal, trainloader_balanced, testloader, NUM_EPOCHS, model_name=\"VGG19-Balanceado\")\n",
    "plot_confusion(targets_vgg_b, preds_vgg_b, classes, \"VGG19 - Balanceado\")\n",
    "\n",
    "# ==================== ResNet50 - Desbalanceado ====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ResNet50 - Dataset DESBALANCEADO\")\n",
    "print(\"=\" * 60)\n",
    "res_unbal = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=1000)\n",
    "res_unbal.load_state_dict(models.resnet50(weights=None).state_dict())\n",
    "res_unbal.fc = nn.Linear(2048, 3)\n",
    "\n",
    "preds_res_u, targets_res_u, tl_res_u, vl_res_u = train_and_evaluate(\n",
    "    res_unbal, trainloader, testloader, NUM_EPOCHS, model_name=\"ResNet50-Desbalanceado\")\n",
    "plot_confusion(targets_res_u, preds_res_u, classes, \"ResNet50 - Desbalanceado\")\n",
    "\n",
    "# ==================== ResNet50 - Balanceado ====================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ResNet50 - Dataset BALANCEADO\")\n",
    "print(\"=\" * 60)\n",
    "res_bal = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=1000)\n",
    "res_bal.load_state_dict(models.resnet50(weights=None).state_dict())\n",
    "res_bal.fc = nn.Linear(2048, 3)\n",
    "\n",
    "preds_res_b, targets_res_b, tl_res_b, vl_res_b = train_and_evaluate(\n",
    "    res_bal, trainloader_balanced, testloader, NUM_EPOCHS, model_name=\"ResNet50-Balanceado\")\n",
    "plot_confusion(targets_res_b, preds_res_b, classes, \"ResNet50 - Balanceado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis:**\n",
    "\n",
    "Al entrenar con el dataset **desbalanceado**, ambos modelos tienden a sesgar sus predicciones hacia las clases mayoritarias (`airplane` y `automobile`), ya que durante el entrenamiento ven 10 veces más muestras de estas clases que de `bird`. Las matrices de confusión muestran que la clase `bird` obtiene una tasa de acierto significativamente menor, con muchas muestras clasificadas erróneamente como `airplane` o `automobile`.\n",
    "\n",
    "Con el dataset **balanceado** mediante data augmentation, las predicciones mejoran notablemente para la clase `bird`, ya que el modelo ahora dispone de un número equivalente de muestras de cada clase. Las matrices de confusión presentan una distribución más homogénea en la diagonal, indicando un rendimiento más equilibrado entre clases.\n",
    "\n",
    "El efecto del balanceo es particularmente visible en métricas como el recall (sensibilidad) de la clase `bird`, que mejora sustancialmente con el dataset balanceado. Además, ResNet50 tiende a generalizar mejor que VGG19 gracias a sus conexiones residuales que facilitan el flujo del gradiente en redes profundas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acPGG1NkK9b5"
   },
   "source": [
    "3. Generate graphs to compare the performance of the implemented networks on the different datasets, focusing on metrics such as training/validation loss, sensitivity, specificity, f1-measure ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT53j6eW9dfL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Almacenar todos los resultados\n",
    "results = {\n",
    "    'VGG19\\nDesbal.': (preds_vgg_u, targets_vgg_u, tl_vgg_u, vl_vgg_u),\n",
    "    'VGG19\\nBalanc.': (preds_vgg_b, targets_vgg_b, tl_vgg_b, vl_vgg_b),\n",
    "    'ResNet50\\nDesbal.': (preds_res_u, targets_res_u, tl_res_u, vl_res_u),\n",
    "    'ResNet50\\nBalanc.': (preds_res_b, targets_res_b, tl_res_b, vl_res_b),\n",
    "}\n",
    "\n",
    "# ==================== Gráfica 1: Training & Validation Loss ====================\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "for name, (_, _, tl, vl) in results.items():\n",
    "    axes[0].plot(range(1, len(tl)+1), tl, marker='o', label=name)\n",
    "    axes[1].plot(range(1, len(vl)+1), vl, marker='s', label=name)\n",
    "\n",
    "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training Loss'); axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Validation Loss'); axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.suptitle('Evolución de la pérdida durante el entrenamiento', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== Gráfica 2: Precision, Recall (Sensitivity), F1-Score por clase ====================\n",
    "metrics_data = []\n",
    "for name, (preds, targets, _, _) in results.items():\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average=None, labels=[0, 1, 2])\n",
    "    for i, cls in enumerate(classes):\n",
    "        metrics_data.append({\n",
    "            'Modelo': name, 'Clase': cls,\n",
    "            'Precision': precision[i],\n",
    "            'Recall (Sensitivity)': recall[i],\n",
    "            'F1-Score': f1[i]\n",
    "        })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "for idx, metric in enumerate(['Precision', 'Recall (Sensitivity)', 'F1-Score']):\n",
    "    ax = axes[idx]\n",
    "    pivot = metrics_df.pivot(index='Clase', columns='Modelo', values=metric)\n",
    "    pivot.plot(kind='bar', ax=ax, rot=0)\n",
    "    ax.set_title(metric, fontsize=13)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.legend(fontsize=7, loc='lower right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Métricas por clase - Comparación de modelos y datasets', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== Gráfica 3: Specificity por clase ====================\n",
    "spec_data = []\n",
    "for name, (preds, targets, _, _) in results.items():\n",
    "    cm_temp = confusion_matrix(targets, preds, labels=[0, 1, 2])\n",
    "    for i, cls in enumerate(classes):\n",
    "        tn = cm_temp.sum() - cm_temp[i, :].sum() - cm_temp[:, i].sum() + cm_temp[i, i]\n",
    "        fp = cm_temp[:, i].sum() - cm_temp[i, i]\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        spec_data.append({'Modelo': name, 'Clase': cls, 'Specificity': specificity})\n",
    "\n",
    "spec_df = pd.DataFrame(spec_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "pivot_spec = spec_df.pivot(index='Clase', columns='Modelo', values='Specificity')\n",
    "pivot_spec.plot(kind='bar', rot=0, ax=ax)\n",
    "ax.set_title('Specificity por clase', fontsize=13)\n",
    "ax.set_ylabel('Specificity')\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== Gráfica 4: Accuracy global ====================\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "accs = {name: accuracy_score(targets, preds) for name, (preds, targets, _, _) in results.items()}\n",
    "colors = ['#e74c3c', '#2ecc71', '#3498db', '#f39c12']\n",
    "bars = ax.bar(accs.keys(), accs.values(), color=colors)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy Global - Comparación de modelos y datasets')\n",
    "ax.set_ylim(0, 1.1)\n",
    "for bar, val in zip(bars, accs.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "            f'{val:.3f}', ha='center', fontsize=11, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== Tabla resumen ====================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RESUMEN DE MÉTRICAS\")\n",
    "print(\"=\" * 80)\n",
    "for name, (preds, targets, _, _) in results.items():\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    print(f\"\\n--- {name.replace(chr(10), ' ')} (Accuracy: {acc:.4f}) ---\")\n",
    "    print(classification_report(targets, preds, target_names=classes, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis comparativo:**\n",
    "\n",
    "Las gráficas permiten observar:\n",
    "\n",
    "* **Training/Validation Loss:** ResNet50 converge más rápido que VGG19 gracias a sus conexiones residuales que facilitan el flujo del gradiente. Con el dataset balanceado, ambos modelos alcanzan valores de loss más estables y generalizan mejor (menor gap entre train y val loss).\n",
    "\n",
    "* **Precision, Recall (Sensitivity) y F1-Score:** La clase `bird` es la más afectada por el desbalance. Con el dataset desbalanceado, el recall de `bird` es significativamente inferior al de las demás clases, indicando que el modelo falla al identificar pájaros (los clasifica como otra clase). Con el balanceo, el recall de `bird` mejora sustancialmente, igualándose más al de las otras clases.\n",
    "\n",
    "* **Specificity:** Mide la capacidad del modelo para no predecir falsamente una clase. Con el dataset desbalanceado, la especificidad de `bird` puede parecer alta (porque el modelo raramente predice `bird`), pero esto es engañoso — el modelo simplemente ignora esa clase. Con el balanceo, la especificidad se mantiene alta pero ahora acompañada de un buen recall.\n",
    "\n",
    "* **Accuracy Global:** ResNet50 tiende a obtener mejor accuracy que VGG19, especialmente con el dataset balanceado, debido a su mayor capacidad de generalización con conexiones residuales. El balanceo del dataset mejora la accuracy en ambos modelos al distribuir equitativamente el aprendizaje entre clases.\n",
    "\n",
    "En conclusión, el uso de data augmentation para balancear el dataset, combinado con fine-tuning de modelos preentrenados, produce clasificadores más robustos y equitativos entre clases. ResNet50 es la arquitectura más recomendable por su convergencia más rápida y mejor generalización."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_qaocy40LqE3",
    "HBUBNSHVMmrc",
    "7sMpLLUKsozk",
    "P2T16QubMmBj"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}