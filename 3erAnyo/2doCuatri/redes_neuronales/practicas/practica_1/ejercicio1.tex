\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish,es-tabla]{babel}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}

\title{\textbf{Ejercicio 1: Backpropagation}}
\author{Jordi Blasco Lozano}
\date{}

\begin{document}
\maketitle

\section*{Planteamiento}
Se considera una red neuronal feedforward con una neurona por capa. La arquitectura es:
\[
 a^{(0)}=x \;\longrightarrow\; (L-2) \;\longrightarrow\; (L-1) \;\longrightarrow\; (L) = \hat{y}.
\]
En cada capa se calcula una preactivación $z^{(l)}$ y una activación $a^{(l)}$.

\subsection*{Datos iniciales}
\begin{itemize}
  \item Entrada: $x = 1$.
  \item Pesos iniciales: $W^{(L-2)} = 0.32$, $W^{(L-1)} = 0.18$, $W^{(L)} = 0.23$.
  \item Sesgos iniciales: $b^{(L-2)} = 0$, $b^{(L-1)} = 0$, $b^{(L)} = 0$.
  \item Tasa de aprendizaje: $\alpha = 0.1$.
  \item Etiqueta (target): $y = 1$.
\end{itemize}

\section*{Notación y fórmulas}
Para cada capa $l$:
\begin{align*}
  z^{(l)} &= W^{(l)}\, a^{(l-1)} + b^{(l)}, \\
  a^{(l)} &= \sigma\bigl(z^{(l)}\bigr).
\end{align*}
Se usa la sigmoide:
\[
\sigma(t)=\frac{1}{1+e^{-t}}, \qquad \sigma'(t)=\sigma(t)\,\bigl(1-\sigma(t)\bigr).
\]
Función de pérdida:
\[
L\,=\,(\hat{y}-y)^2.
\]
Al trabajar con un solo ejemplo, ``loss'' y ``cost'' coinciden numéricamente.

\section{Apartado 1: Forward pass (cálculo de $z$, $a$ y $\hat{y}$)}
En el forward pass se calculan las activaciones desde la entrada hasta la salida.

\subsection*{Capa $L-2$}
\begin{align*}
z^{(L-2)} &= W^{(L-2)}\,x + b^{(L-2)} 
          = 0.32\cdot 1 + 0
          = 0.32,\\
 a^{(L-2)} &= \sigma\bigl(z^{(L-2)}\bigr)
           = \frac{1}{1+e^{-0.32}}
           \approx \frac{1}{1+0.7261}
           \approx 0.5793.
\end{align*}

\subsection*{Capa $L-1$}
\begin{align*}
z^{(L-1)} &= W^{(L-1)}\,a^{(L-2)} + b^{(L-1)}
          = 0.18\cdot 0.5793 + 0
          \approx 0.1043,\\
 a^{(L-1)} &= \sigma\bigl(z^{(L-1)}\bigr)
           = \frac{1}{1+e^{-0.1043}}
           \approx \frac{1}{1+0.9009}
           \approx 0.5260.
\end{align*}

\subsection*{Capa de salida $L$}
\begin{align*}
z^{(L)} &= W^{(L)}\,a^{(L-1)} + b^{(L)}
       = 0.23\cdot 0.5260 + 0
       \approx 0.1210,\\
\hat{y} = a^{(L)} &= \sigma\bigl(z^{(L)}\bigr)
                = \frac{1}{1+e^{-0.1210}}
                \approx \frac{1}{1+0.8860}
                \approx 0.5302.
\end{align*}

\subsection*{Pérdida (loss)}
\begin{align*}
L &= (\hat{y}-y)^2
  = (0.5302-1)^2
  = (-0.4698)^2
  \approx 0.2207.
\end{align*}

\subsection*{Resumen del forward pass}
\begin{table}[h]
\centering
\begin{tabular*}{0.8\linewidth}{@{\extracolsep{\fill}}cccc@{}}
\toprule
Capa & $z^{(l)}$ & $a^{(l)}$ & Comentario \\ \midrule
$L-2$ & $0.32$ & $0.5793$ & $a^{(0)}=x=1$ \\
$L-1$ & $0.1043$ & $0.5260$ & $\sigma(z)$ \\
$L$ & $0.1210$ & $\hat{y}=0.5302$ & salida \\ \bottomrule
\end{tabular*}
\end{table}

\section{Apartado 2: Backward pass (gradientes y actualización)}
El objetivo es actualizar $W^{(L)}$ y $b^{(L-1)}$ mediante descenso por gradiente.

\subsection*{Mapa de dependencias (para justificar la regla de la cadena)}
\[
W^{(L)} \to z^{(L)} \to \hat{y} \to L,
\qquad
b^{(L-1)} \to z^{(L-1)} \to a^{(L-1)} \to z^{(L)} \to \hat{y} \to L.
\]
Para simplificar, definimos el ``error'' (delta) por capa:
\[
\delta^{(l)} := \frac{\partial L}{\partial z^{(l)}}.
\]
Los gradientes se calculan con los parámetros del forward actual (antes de aplicar la actualización).

\subsection*{Capa de salida $L$}
\paragraph{1) Derivada de la pérdida respecto a la salida}
\begin{align*}
\frac{\partial L}{\partial \hat{y}} &= \frac{\partial}{\partial \hat{y}}(\hat{y}-y)^2
= 2(\hat{y}-y)
=2(0.5302-1)
\approx -0.9396.
\end{align*}

\paragraph{2) Delta de la capa de salida}
\begin{align*}
\delta^{(L)} = \frac{\partial L}{\partial z^{(L)}}
&= \frac{\partial L}{\partial \hat{y}}\,\frac{\partial \hat{y}}{\partial z^{(L)}}
= \frac{\partial L}{\partial \hat{y}}\,\sigma'(z^{(L)})\\
&= (-0.9396)\,\bigl(\hat{y}(1-\hat{y})\bigr)
= (-0.9396)\,(0.5302\cdot 0.4698)
\approx (-0.9396)\cdot 0.2491
\approx -0.2340.
\end{align*}

\paragraph{3) Gradiente respecto a $W^{(L)}$}
\leavevmode\\
Como $z^{(L)} = W^{(L)}a^{(L-1)} + b^{(L)}$, se tiene
\(
\frac{\partial z^{(L)}}{\partial W^{(L)}} = a^{(L-1)}
\).
Por tanto:
\begin{align*}
\frac{\partial L}{\partial W^{(L)}}
&= \frac{\partial L}{\partial z^{(L)}}\,\frac{\partial z^{(L)}}{\partial W^{(L)}}
= \delta^{(L)}\,a^{(L-1)}
\approx (-0.2340)\cdot 0.5260
\approx -0.1231.
\end{align*}

\paragraph{4) Actualización de $W^{(L)}$}
\begin{align*}
W^{(L)}_{\text{nuevo}}
&= W^{(L)} - \alpha\,\frac{\partial L}{\partial W^{(L)}}
= 0.23 - 0.1\cdot(-0.1231)
\approx 0.2423.
\end{align*}

\subsection*{Capa $L-1$ (para actualizar $b^{(L-1)}$)}
\paragraph{5) Propagación del delta a la capa anterior}

\[
\delta^{(L-1)} = \frac{\partial L}{\partial z^{(L-1)}}
= \underbrace{\frac{\partial L}{\partial z^{(L)}}}_{\delta^{(L)}}\,\underbrace{\frac{\partial z^{(L)}}{\partial a^{(L-1)}}}_{W^{(L)}}\,\underbrace{\frac{\partial a^{(L-1)}}{\partial z^{(L-1)}}}_{\sigma'(z^{(L-1)})}.
\]
Sustituyendo valores del forward (con $W^{(L)}=0.23$ antes de actualizar):
\begin{align*}
\delta^{(L-1)}
&= \delta^{(L)}\,W^{(L)}\,\sigma'(z^{(L-1)})\\
&\approx (-0.2340)\cdot 0.23 \cdot \bigl(a^{(L-1)}(1-a^{(L-1)})\bigr)\\
&\approx (-0.2340)\cdot 0.23 \cdot (0.5260\cdot 0.4740)\\
&\approx (-0.2340)\cdot 0.23 \cdot 0.2493
\approx -0.0134.
\end{align*}

\paragraph{6) Gradiente y actualización de $b^{(L-1)}$}
\leavevmode\\
Como $z^{(L-1)} = W^{(L-1)}a^{(L-2)} + b^{(L-1)}$, se cumple
\(
\frac{\partial z^{(L-1)}}{\partial b^{(L-1)}}=1
\), y por tanto:
\[
\frac{\partial L}{\partial b^{(L-1)}} = \delta^{(L-1)} \approx -0.0134.
\]
La actualización queda:
\begin{align*}
 b^{(L-1)}_{\text{nuevo}} &= b^{(L-1)} - \alpha\,\frac{\partial L}{\partial b^{(L-1)}}
= 0 - 0.1\cdot(-0.0134)
\approx 0.00134.
\end{align*}

\section*{Resumen final}
\begin{table}[h]
\centering
\begin{tabular*}{0.8\linewidth}{@{\extracolsep{\fill}}ll@{}}
\toprule
Magnitud & Valor \\ \midrule
$\hat{y}$ & $0.5302$ \\
$L=(\hat{y}-y)^2$ & $0.2207$ \\
$\frac{\partial L}{\partial W^{(L)}}$ & $-0.1231$ \\
$W^{(L)}_{\text{nuevo}}$ & $0.2423$ \\
$\frac{\partial L}{\partial b^{(L-1)}}$ & $-0.0134$ \\
$b^{(L-1)}_{\text{nuevo}}$ & $0.00134$ \\ \bottomrule
\end{tabular*}
\end{table}

\end{document}

