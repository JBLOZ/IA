%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage[T1]{fontenc}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% Eliminar el texto de "Proceedings of ICML..." de la primera página
\makeatletter
\renewcommand{\ICML@appearing}{}
\makeatother

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

% Custom table spacing
\usepackage{tabla}

% Pie de página personalizado en la última página
\usepackage{lastpage}
\makeatletter
\fancypagestyle{lastpagestyle}{%
  \fancyhf{}
  \fancyhead{}
  \chead{}
  \cfoot{\thepage\\[0.3cm]\small\hypertarget{author1}{}\textsuperscript{1}Jordi Blasco Lozano -- DNI: 74527208D -- jordiblloz@gmail.com}
  \renewcommand{\headrulewidth}{1pt}
}
\AtEndDocument{\thispagestyle{lastpagestyle}}
\makeatother




% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Predicción de Clase de Animal mediante Clasificadores Bayesianos y k-NN}

\begin{document}

\twocolumn[
\icmltitle{Predicción de Clase de Animal mediante Clasificadores Bayesianos, \\ Estimadores No Paramétricos y k-NN}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

\begin{icmlauthorlist}
\icmlauthor{Jordi Blasco Lozano\hyperlink{author1}{\textsuperscript{1}}}{}
\end{icmlauthorlist}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Bayesian Classifiers, k-NN, Parzen Windows, Multiclass Classification}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

\begin{abstract}
Este trabajo aborda la práctica 2 en la asignatura de Fundamentos del Aprendizaje Automático empleando el dataset Zoo de UCI, aplicando seis algoritmos de clasificación: Naive Bayes Gaussiano, MLE Multivariante, Histogram Bayes, Parzen Windows, k-NN Density Bayes y k-NN Rule. Se analiza su rendimiento en una tarea real de clasificación multiclase (7 clases) con características binarias y clases minoritarias.
\end{abstract}

\section{Introducción y justificación del dataset}
\label{introduction}

He seleccionado el \textbf{dataset Zoo} porque me parece muy interesante y contiene: clasificación multiclase (7 clases: mamífero, ave, reptil, pez, anfibio, invertebrado, insecto), 16 atributos binarios interpretables, y tamaño adecuado (101 instancias originales) para observar comportamientos de métodos paramétricos y no paramétricos.

Dado el desbalance original (41 mamíferos vs 4 anfibios y un \textbf{meanIR} de 4.84), generé sintéticamente nuevos animales mediante investigación profunda usando IA, añadiendo nuevos animales con sus características, alcanzando las 287 instancias con 41 ejemplares por clase. Esto permite evaluar cómo el balanceo afecta el rendimiento de cada modelo, especialmente aquellos sensibles a datos escasos como el MLE FULL.

\section{Dataset y Análisis Exploratorio}
\label{dataset}

\textbf{Dataset original:} 101 instancias, 17 atributos (15 binarios + 1 numérico + 1 clase), 7 clases desbalanceadas (mamíferos: 41, aves: 20, reptiles: 5, peces: 13, anfibios: 4, invertebrados: 8, insectos: 10).\\ \textbf{Dataset balanceado:} 287 instancias con 41 ejemplares por clase, generados sintéticamente mediante investigación de características específicas de cada tipo de animal.

Los 17 atributos incluyen \textbf{15 características binarias}: pelo, plumas, huevos, leche, vuela, acuático, depredador, dientes, columna vertebral, respira, venenoso, aletas, cola, doméstico, tamaño grande; \textbf{1 atributo numérico}: patas; y \textbf{1 atributo de tipo clase} (valores 1-7 codificados como los grupos de animales anteriores).

\begin{figure}[h]
\centering

\vspace{-0.5cm}
\caption{Disposición del dataset balanceado}
\label{fig:tsne}

En la anterior figura podemos ver cómo están configurados los clusters de las clases de animales respecto a sus características usando embeddings no lineales.
\end{figure}



\section{Metodología: Particionado y Validación}
\label{methodology}

\textbf{Validación Cruzada 5-fold Estratificada:} Se aplica validación cruzada 5-fold estratificada sobre todo el conjunto de datos. En cada fold, el conjunto se divide en 80\% entrenamiento y 20\% test, manteniendo las proporciones de clases. El proceso se repite 5 veces, de modo que cada partición actúa como test una vez. Para cada modelo se obtienen 5 métricas independientes, cuya media y desviación estándar reflejan el rendimiento y la estabilidad del modelo.\linebreak
\textbf{Optimización de Hiperparámetros:} En los modelos que requieren selección de hiperparámetros, se evalúan todos los valores candidatos en cada uno de los 5 folds. Se calcula el F1-macro promedio de cada hiperparámetro y se selecciona el que maximiza esta métrica.\linebreak
\textbf{Métricas:} F1-macro (métrica principal), accuracy (para comparación general) y matrices de confusión (para analizar errores por clase).

\section{Modelos Implementados}
\label{models}

\textbf{Seis clasificadores:} (1) Naive Bayes Gaussiano, (2) MLE Multivariante con covarianza completa, (3) Histogram Bayes, (4) Parzen Windows con kernel Gaussiano, (5) k-NN Density Bayes, (6) k-NN Rule.

\section{Resultados}
\label{results}

Las \cref{tab:results_desbalanced,tab:results_balanced} muestran la media y desviación estándar de Accuracy y F1-macro obtenidas en los para cada dataset.



\begin{table}[h]
\caption{Resultados en dataset desbalanceado (CV 5-fold)}
\label{tab:results_desbalanced}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{\textbf{Accuracy}} & \multicolumn{2}{c}{\textbf{F1-macro}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Modelo} & \textbf{mean} & \textbf{std} & \textbf{mean} & \textbf{std} \\
\midrule
\multicolumn{5}{l}{\textit{Modelos Paramétricos}} \\
Naive Bayes & 0.96 & 0.06 & 0.90 & 0.14 \\
MLE Full & 0.81 & 0.04 & 0.58 & 0.05 \\
\midrule
\multicolumn{5}{l}{\textit{Modelos No Paramétricos}} \\
Histogram Bayes & 0.46 & 0.06 & 0.17 & 0.10 \\
Parzen ($h=0.5$) & 0.98 & 0.02 & 0.91 & 0.12 \\
k-NN Density ($k=1$) & 0.98 & 0.02 & 0.90 & 0.12 \\
k-NN Rule ($k=1$) & 0.98 & 0.02 & 0.91 & 0.12 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{tablacompacta}[h]
\caption{Resultados en dataset balanceado (CV 5-fold)}
\label{tab:results_balanced}
\vskip 0.05in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
 & \multicolumn{2}{c}{\textbf{Accuracy}} & \multicolumn{2}{c}{\textbf{F1-macro}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Modelo} & \textbf{mean} & \textbf{std} & \textbf{mean} & \textbf{std} \\
\midrule
\multicolumn{5}{l}{\textit{Modelos Paramétricos}} \\
Naive Bayes & 0.88 & 0.07 & 0.87 & 0.08 \\
MLE Full & 0.89 & 0.05 & 0.89 & 0.05 \\
\midrule
\multicolumn{5}{l}{\textit{Modelos No Paramétricos}} \\
Histogram Bayes & 0.15 & 0.01 & 0.06 & 0.02 \\
Parzen ($h=0.5$) & 0.93 & 0.03 & 0.93 & 0.03 \\
k-NN Density ($k=1$) & 0.93 & 0.03 & 0.93 & 0.04 \\
k-NN Rule ($k=3$) & 0.92 & 0.05 & 0.91 & 0.05 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}

\end{tablacompacta}
\vspace{0.1cm}
\subsection{Análisis detallado por modelo}

\subsubsection{Naive Bayes Gaussiano}
\textbf{Desbalanceado:} Acc = 0.96 $\pm$ 0.06, F1 = 0.90 $\pm$ 0.14; \textbf{Balanceado:} Acc = 0.88 $\pm$ 0.07, F1 = 0.87 $\pm$ 0.08

Naive Bayes obtiene excelente rendimiento en el dataset desbalanceado (F1=0.90), demostrando robustez ante clases minoritarias gracias a estimar solo $2d$ parámetros por clase ($\mu_{y,i}$ y $\sigma^2_{y,i}$). Con el dataset balanceado, el rendimiento baja ligeramente (F1=0.87), pero la desviación se reduce notablemente (0.14$\rightarrow$0.08), indicando mayor estabilidad al disponer de más datos por clase. La matriz de confusión revela que las principales confusiones ocurren entre reptiles-anfibios (13 casos) y anfibios-reptiles (11 casos), reflejando el solapamiento real de características binarias entre estas clases. A pesar de la asunción "naive" de independencia, el modelo captura eficazmente las distribuciones de clase, siendo especialmente preciso con mamíferos, aves e invertebrados (recall $\approx$ 1.00).

\subsubsection{MLE Multivariante (Full Bayesian Gaussian)}
\textbf{Desbalanceado:} Acc = 0.81 $\pm$ 0.04, F1 = 0.58 $\pm$ 0.05; \textbf{Balanceado:} Acc = 0.89 $\pm$ 0.05, F1 = 0.89 $\pm$ 0.05

Este modelo exhibe la mejora más dramática con el balanceo (F1: 0.58 $\rightarrow$ 0.89, +53\%). En el dataset desbalanceado, la matriz de confusión revela colapso total en clases minoritarias: 5 reptiles clasificados como mamíferos, 4 anfibios como mamíferos, resultando en precision cero para estas clases. La causa es que estimar la covarianza completa $16 \times 16$ (136 parámetros únicos por clase) con solo 3-5 ejemplos produce matrices mal condicionadas, generando predicciones inestables. Con el balanceo (41 ejemplos/clase), el modelo estima correctamente las correlaciones entre características y alcanza F1=0.89, igualando a Naive Bayes y demostrando que capturar dependencias completas es valioso cuando hay datos suficientes.

\subsubsection{Histogram Bayes}
\textbf{Desbalanceado:} Acc = 0.46 $\pm$ 0.06, F1 = 0.17 $\pm$ 0.10; \textbf{Balanceado:} Acc = 0.15 $\pm$ 0.01, F1 = 0.06 $\pm$ 0.02

Este modelo sufre el peor rendimiento, demostrando brutalmente la maldición de la dimensionalidad. Con $2^{16}=65,536$ bins posibles pero solo 287 muestras (41/clase), la mayoría de bins quedan vacíos, asignando $p(\mathbf{x}|y)=0$ a patrones no vistos y forzando predicciones por defecto. La matriz de confusión balanceada muestra el colapso: 283/287 muestras clasificadas como mamíferos (recall=1.00 mamífero, 0.00 resto). El accuracy paradójicamente empeora con el balanceo (0.46$\rightarrow$0.15) porque en desbalanceado, predecir "mamífero" siempre acierta 40\% del tiempo, mientras que con clases equiprobables solo acierta 14\%. El F1-macro permanece consistentemente bajo (0.17$\rightarrow$0.06) porque evalúa todas las clases por igual. Los histogramas requieren $N \propto b^d$ muestras; con $d=16$ y $N=287$, el método es fundamentalmente inadecuado.

\subsubsection{Parzen Windows}
\textbf{Desbalanceado:} Acc = 0.98 $\pm$ 0.02, F1 = 0.91 $\pm$ 0.12; \textbf{Balanceado:} Acc = 0.93 $\pm$ 0.03, F1 = 0.93 $\pm$ 0.03

Parzen Windows alcanza el mejor rendimiento global, especialmente tras balanceo (F1=0.93), con bandwidth óptimo $h=0.5$ que proporciona suavizado continuo sin sobreajuste. En el dataset desbalanceado, obtiene F1=0.91 con alta varianza (std=0.12), reflejando sensibilidad a la composición de los folds con clases minoritarias. El balanceo estabiliza dramáticamente el modelo (std: 0.12$\rightarrow$0.03), demostrando que el kernel Gaussiano funciona óptimamente con datos equilibrados. La matriz de confusión muestra confusiones concentradas en reptiles-anfibios (6+4=10 casos) e insectos dispersos entre múltiples clases, coherentes con solapamiento de características binarias. A diferencia de histogramas, el suavizado continuo mitiga la maldición de dimensionalidad prestando densidad de vecinos cercanos sin crear discontinuidades artificiales. La búsqueda de hiperparámetros revela que $h$ pequeños (0.05-0.5) mantienen F1$\approx$0.93, mientras que $h$ grandes (1.5-2.0) degradan a F1$\approx$0.60, confirmando importancia de localidad.

\subsubsection{k-NN Density Bayes}
\textbf{Desbalanceado:} Acc = 0.98 $\pm$ 0.02, F1 = 0.90 $\pm$ 0.12; \textbf{Balanceado:} Acc = 0.93 $\pm$ 0.03, F1 = 0.93 $\pm$ 0.04

Con $k=1$ óptimo, este modelo iguala prácticamente a Parzen Windows en F1, sugiriendo que en este dataset la asignación de densidad local adaptativa funciona óptimamente con el vecino más cercano. En desbalanceado, obtiene F1=0.90 con alta varianza (0.12), similar a Parzen, reflejando sensibilidad a clases minoritarias. El balanceo estabiliza el modelo (std: 0.12$\rightarrow$0.04) y mejora F1 (0.90$\rightarrow$0.93). La búsqueda de $k$ revela degradación con valores mayores: $k=3$ produce F1=0.92, $k≥5$ baja a F1 (0.83-0.88), confirmando que promediar muchos vecinos diluye información local crítica. La matriz de confusión muestra confusiones reptiles-anfibios (1+10=11 casos), coherentes con Parzen. Aunque el método estima densidades explícitamente mediante $\hat{p}(\mathbf{x}|y) = k/(NV_k(\mathbf{x}))$, con $k=1$ se comporta esencialmente como asignación directa, explicando su equivalencia con k-NN Rule.

\subsubsection{k-NN Rule}
\textbf{Desbalanceado:} Acc = 0.98 $\pm$ 0.02, F1 = 0.91 $\pm$ 0.12; \textbf{Balanceado:} Acc = 0.92 $\pm$ 0.05, F1 = 0.91 $\pm$ 0.05

Este método clásico obtiene resultados excelentes, especialmente en desbalanceado (F1=0.91), con $k$ óptimo variando según balanceo: $k=1$ en desbalanceado, $k=3$ en balanceado. La simplicidad del método (asignación directa por votación mayoritaria sin estimar densidades) resulta ventajosa: con $k=1$ en desbalanceado, iguala a Parzen y k-NN Density, confirmando que la estructura espacial está bien definida. En balanceado, $k=3$ aporta ligero suavizado (F1=0.91 vs 0.93 de Parzen), con matriz de confusión mostrando confusiones reptiles-anfibios (2+11=13 casos) coherentes con otros métodos. La búsqueda revela que $k$ grandes degradan rendimiento: $k=11$ baja a F1$\approx$0.51-0.89, confirmando que promediar muchos vecinos borra fronteras de decisión locales. La alta varianza en balanceado (std=0.05 vs 0.03 de Parzen) indica una mayor sensibilidad a la composición de los folds.

\section{Discusión y Conclusiones}
\label{discussion}

\textbf{Comparación general de enfoques:} Los modelos paramétricos muestran comportamientos complementarios: Naive Bayes es robusto con clases minoritarias (F1=0.90 desbalanceado) estimando solo $2d$ parámetros por clase, mientras MLE Full colapsa en desbalanceado (F1=0.58) pero alcanza F1=0.89 con balanceo, igualando a NB. Esto confirma el equilibrio sesgo-varianza: modelos simples son robustos con datos escasos; los complejos necesitan más muestras pero capturan correlaciones valiosas. Entre no paramétricos, Histogram Bayes falla completamente (F1=0.06-0.17) por la maldición de dimensionalidad: $2^{16}=65,536$ bins con 287 muestras resultan en colapso predictivo. Los tres métodos de vecinos cercanos (Parzen, k-NN Density, k-NN Rule) logran rendimiento excepcional (F1=0.91-0.93 balanceado) mediante suavizado continuo o asignación directa. Parzen con $h=0.5$ obtiene máximo F1=0.93 y mínima varianza (std=0.03), confirmándose como mejor modelo global. k-NN Density con $k=1$ iguala a Parzen (F1=0.93), mientras k-NN Rule con $k=3$ queda ligeramente atrás (F1=0.91), sugiriendo que votación mayoritaria es menos precisa que estimación de densidad en este problema.

\textbf{Impacto del desbalance y balanceo:} El desbalance original (meanIR=4.84) expone brutalmente debilidades de modelos complejos: MLE Full colapsa con clases minoritarias (F1=0.58), generando precision cero para reptiles y anfibios al no poder estimar covarianzas con 3-5 ejemplos. El balanceo sintético ($101\rightarrow287$ instancias) mejora el F1 de MLE Full en un 53\%, alcanzando el 0.89. En Histogram Bayes paradójicamente el accuracy empeora: 0.46$\rightarrow$0.15, pero F1 permanece consistente en colapso (0.17$\rightarrow$0.06), confirmando que predice clase mayoritaria por defecto. Los métodos de vecinos cercanos mejoran con balanceo: +0.2 aprox en F1 y notándose sobre todo en las std que se reducen dramáticamente (un 0.8 aprox), revelando que con 41 ejemplos/clase las estimaciones locales son robustas.

\textbf{Métricas:} El contraste accuracy vs F1-macro es crucial: Histogram obtiene accuracy=0.46 pero F1=0.17, prediciendo predominantemente "mamífero". El accuracy engaña porque predecir clase mayoritaria acierta 40\% en desbalanceado; F1-macro revela el colapso real al promediar todas las clases equitativamente.

\textbf{Conclusiones finales:} Este trabajo confirma empíricamente tres principios del aprendizaje automático: (1) el equilibrio entre complejidad del modelo y cantidad de datos (MLE Full con balanceo mejora un 53\%; Naive Bayes se mantiene); (2) la maldición de dimensionalidad con Histogram colapsando; y (3) la importancia crítica de estabilidad (balanceo reduce desviaciones). \textbf{Parzen Windows con $h=0.5$} emerge como mejor modelo, combinando alto rendimiento y mínima varianza. \linebreak \textbf{Lecciones clave:} F1-macro es la métrica esencial en multiclase desbalanceado; la asunción "naive" es sorprendentemente efectiva sugiriendo independencia entre características binarias; confusiones reptiles-anfibios (10-13 casos) son biológicamente coherentes dado solapamiento de atributos (huevos, venenoso) 
como podemos ver en \cref{fig:tsne}; y el balanceo sintético mediante investigación profunda produce datos válidos que estabilizan modelos complejos.



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
