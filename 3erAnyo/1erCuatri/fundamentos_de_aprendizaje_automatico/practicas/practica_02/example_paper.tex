%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Predicción de Clase de Animal mediante Clasificadores Bayesianos y k-NN}

\begin{document}

\twocolumn[
\icmltitle{Predicción de Clase de Animal mediante Clasificadores Bayesianos, \\ Estimadores No Paramétricos y k-NN}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

\begin{icmlauthorlist}
\icmlauthor{Jordi Blasco Lozano}{}
\end{icmlauthorlist}

\icmlaffiliation{uji}{Fundamentos del Aprendizaje Automático, Universitat Jaume I, Castellón, España}

\icmlcorrespondingauthor{Jordi Blasco Lozano}{74527208D}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Bayesian Classifiers, k-NN, Parzen Windows, Multiclass Classification}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

\begin{abstract}
Este trabajo aborda la práctica 2 en la asignatura de Fundamentos del Aprendizaje Automático empleando el dataset Zoo de UCI, aplicando seis algoritmos de clasificación: Naive Bayes Gaussiano, MLE Multivariante, Histogram Bayes, Parzen Windows, k-NN Density Bayes y k-NN Rule. Se analiza su rendimiento en una tarea real de clasificación multiclase (7 clases) con características binarias y clases minoritarias. Los resultados revelan que Naive Bayes, Parzen Windows y k-NN Rule logran clasificación perfecta, mientras que métodos más complejos fallan debido a la maldición de la dimensionalidad y la escasez de datos en clases minoritarias.
\end{abstract}

\section{Introducción y justificación del dataset}
\label{introduction}

He seleccionado el \textbf{dataset Zoo} de UCI para esta práctica porque cumple estrictamente con los requerimientos del enunciado y permite aplicar eficazmente todos los métodos estudiados en la asignatura. Las razones principales son:

\begin{itemize}
    \item \textbf{Clasificación multiclase:} El enunciado exige explícitamente abordar problemas con múltiples clases, no solo binarios. El dataset Zoo tiene 7 clases diferentes (mamífero, ave, reptil, pez, anfibio, invertebrado, insecto).
    \item \textbf{Características interpretables:} 16 atributos binarios (presencia/ausencia de características físicas o comportamentales) que facilitan el análisis y la aplicación de estimadores de densidad.
    \item \textbf{Tamaño adecuado:} 101 instancias, suficientes para validación pero pequeñas para observar comportamientos de los métodos no paramétricos con datos limitados.
\end{itemize}

El objetivo es comparar el rendimiento de clasificadores bayesianos paramétricos versus estimadores no paramétricos de densidad, analizando sus fortalezas y debilidades en clasificación multiclase con datos de tamaño moderado.

\section{Dataset y Análisis Exploratorio}
\label{dataset}

El \textbf{dataset Zoo} consta de:
\begin{itemize}
    \item \textbf{Tamaño:} 101 instancias (animales)
    \item \textbf{Dimensionalidad:} 16 atributos binarios (0/1) que describen características físicas y comportamentales
    \item \textbf{Clases:} 7 tipos de animales (multiclase):
    \begin{itemize}
        \item Mamífero (mammal): 41 muestras (40.6\%)
        \item Ave (bird): 20 muestras (19.8\%)
        \item Reptil (reptile): 5 muestras (5.0\%)
        \item Pez (fish): 13 muestras (12.9\%)
        \item Anfibio (amphibian): 4 muestras (4.0\%)
        \item Invertebrado (invertebrate): 8 muestras (7.9\%)
        \item Insecto (insect): 10 muestras (9.9\%)
    \end{itemize}
\end{itemize}

\subsection{Características del dataset}

El dataset presenta una distribución de clases moderadamente desbalanceada, con mamíferos dominando (40.6\%) y anfibios siendo la clase minoritaria (4.0\%). Esta distribución es más realista que un desbalance extremo y permite evaluar cómo los diferentes métodos manejan clases con pocas muestras.

Las 16 características binarias incluyen atributos como: tiene pelo, pone huevos, vuela, acuático, depredador, con dientes, columna vertebral, respira, venenoso, con aletas, número de patas, cola, doméstico, tamaño, etc.

\subsection{Preprocesamiento}

No se requiere normalización porque todas las características son binarias (0 o 1). Los datos están completos sin valores faltantes. El único preprocesamiento necesario es la codificación de las etiquetas de clase a valores numéricos (0-6) para los algoritmos.

\section{Metodología: Particionado y Validación}
\label{methodology}

\subsection{División Train-Test (80\%-20\%)}
He implementado una división estratificada del dataset, mi conjunto de entrenamiento será de 80 muestras y el de prueba de 21. 

La estratificación garantiza que la proporción de clases se mantenga en ambos conjuntos. Dada la distribución desbalanceada (especialmente las clases minoritarias con 4-5 ejemplos), la estratificación es crucial para asegurar que todas las clases estén representadas en ambos conjuntos.

\subsection{Validación Cruzada para Hiperparámetros}

Para la optimización de hiperparámetros (bandwidth en Parzen, k en k-NN), utilizo validación cruzada estratificada de 5 folds únicamente sobre el conjunto de entrenamiento. Esto evita contaminación de datos y garantiza que las métricas del conjunto de prueba sean imparciales, finalmente para el conjunto de test utilizaremos los hiperparametros que mas precisión obtengan sobre el conjunto de entrenamiento.

\subsection{Metodología de Evaluación}
Para medir cada modelo utilizaremos las métricas siguientes:

\begin{itemize}
    
    \item \textbf{F1-macro:} Promedia el F1-score de todas las clases, tratándolas con igual importancia, dado nuestro dataset desbalanceado será nuestra métrica base.
    \item \textbf{Metricas secundarias:} Precision, recall para observar como de bueno es cada modelo en falsos positivos y falsos negativos de forma separada.
    \item \textbf{Matrices de confusión:} Para análisis detallado clase por clase.
    
\end{itemize}

\section{Modelos Implementados}
\label{models}

He implementado y comparado seis clasificadores:

\begin{enumerate}
    \item \textbf{Naive Bayes Gaussiano}: Clasificador paramétrico con independencia condicional
    \item \textbf{MLE Multivariante (Full Bayesian Gaussian)}: Estimación máxima verosimilitud con covarianza completa
    \item \textbf{Histogram Bayes}: Estimador no paramétrico basado en histogramas
    \item \textbf{Parzen Windows}: Estimador no paramétrico con kernel Gaussiano
    \item \textbf{k-NN Density Bayes}: Estimador no paramétrico basado en densidad local
    \item \textbf{k-NN Rule}: Regla de los k vecinos más cercanos clásica
\end{enumerate}

\section{Resultados}
\label{results}

He evaluado los seis modelos en el conjunto de prueba de 21 muestras. La \cref{tab:results_test} muestra los resultados principales.

\begin{table}[h]
\caption{Resultados en conjunto de prueba (21 muestras, 7 clases)}
\label{tab:results_test}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
\textbf{Modelo} & \textbf{Accuracy} & \textbf{F1-macro} \\
\midrule
Naive Bayes & 1.0000 & 1.0000 \\
MLE Full & 0.7143 & 0.4563 \\
Histogram Bayes & 0.3810 & 0.0788 \\
Parzen Bayes (h=0.1) & 1.0000 & 1.0000 \\
k-NN Density (k=11) & 0.4762 & 0.5714 \\
k-NN Rule (k=1) & 1.0000 & 1.0000 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[h]
\caption{Validación cruzada 5-fold en conjunto de entrenamiento}
\label{tab:results_cv}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
\textbf{Modelo} & \textbf{F1-CV (mean)} & \textbf{F1-CV (std)} \\
\midrule
Naive Bayes & 0.8505 & 0.1357 \\
MLE Full & 0.5329 & 0.1021 \\
Histogram Bayes & 0.2474 & 0.1277 \\
Parzen (h=0.1) & 0.8648 & -- \\
k-NN Density (k=11) & 0.5664 & -- \\
k-NN Rule (k=1) & 0.8267 & -- \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Análisis detallado por modelo}

\subsubsection{Naive Bayes Gaussiano}
\textbf{Test:} Accuracy = 1.0, F1-macro = 1.0

\textbf{Fundamento teórico:} Naive Bayes aplica el teorema de Bayes con la asunción de independencia condicional entre características:
\[
P(y|x_1, \ldots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i|y)
\]

Para cada clase $y$ y característica $i$, estima $\mu_{y,i}$ y $\sigma^2_{y,i}$ asumiendo distribución Gaussiana:
\[
P(x_i|y) = \frac{1}{\sqrt{2\pi\sigma^2_{y,i}}} \exp\left(-\frac{(x_i-\mu_{y,i})^2}{2\sigma^2_{y,i}}\right)
\]

\textbf{Análisis:} Naive Bayes logra clasificación perfecta en el conjunto de prueba, clasificando correctamente las 21 muestras. A pesar de la asunción "naive" de independencia entre características, el modelo funciona excepcionalmente bien. Esto sugiere que las características binarias están relativamente poco correlacionadas y que las distribuciones Gaussianas aproximan bien los datos binarios en este caso. La asunción de independencia raramente se cumple en la práctica, pero en este caso no penaliza el rendimiento.

\subsubsection{MLE Multivariante (Full Bayesian Gaussian)}
\textbf{Test:} Accuracy = 0.7143, F1-macro = 0.4563

\textbf{Fundamento teórico:} A diferencia de Naive Bayes, este método \textbf{no asume independencia} entre características. Estima la matriz de covarianza completa $\Sigma_y$ para cada clase:
\[
P(\mathbf{x}|y) = \frac{1}{(2\pi)^{d/2}|\Sigma_y|^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_y)^T\Sigma_y^{-1}(\mathbf{x}-\boldsymbol{\mu}_y)\right)
\]

Donde $\boldsymbol{\mu}_y$ es el vector de medias y $\Sigma_y$ es la matriz de covarianza $d \times d$ para la clase $y$.

\textbf{Análisis:} El modelo multivariante con covarianza completa falla significativamente. Con $d$ características (dimensiones), la matriz de covarianza $\Sigma_y$ es una matriz $d \times d$ simétrica positiva definida, lo que implica que se deben estimar $\frac{d(d+1)}{2}$ parámetros únicos por clase (136 parámetros con $d=16$), ya que los elementos fuera de la diagonal principal se duplican. Este número crece cuadráticamente con $d$, por lo que en espacios de alta dimensión se necesita un gran número de muestras $N_y$ por clase para obtener estimaciones estables. Para clases con 3-6 ejemplos en entrenamiento, esto es imposible, resultando en matrices de covarianza singulares o mal condicionadas porque no hay suficientes datos para capturar las varianzas y covarianzas.

\subsubsection{Histogram Bayes}
\textbf{Test:} Accuracy = 0.3810, F1-macro = 0.0788

\textbf{Fundamento teórico:} Estimador no paramétrico que divide el espacio de características en bins (histogramas) y estima la densidad como:
\[
\hat{p}(\mathbf{x}|y) = \frac{\text{count}(\mathbf{x} \in \text{bin})}{n_y \cdot V_{\text{bin}}}
\]

Donde $n_y$ es el número de ejemplos de la clase $y$ y $V_{\text{bin}}$ es el volumen del bin.

\textbf{Análisis:} El peor rendimiento de todos los modelos, evidenciando la maldición de la dimensionalidad. Con $d$ dimensiones y $b$ bins por dimensión, se necesitan $b^d$ bins. Con 16 características binarias, hay $2^{16} = 65536$ posibles combinaciones (bins). Con solo 101 muestras totales, la mayoría de bins están vacíos, resultando en un colapso total del modelo que predice solo la clase mayoritaria (mamíferos).

\subsubsection{Parzen Windows}
\textbf{Test:} Accuracy = 1.0, F1-macro = 1.0

\textbf{Fundamento teórico:} Estimador no paramétrico que usa un kernel para suavizar la estimación de densidad. En nuestra implementación, utilizamos un \textbf{kernel Gaussiano} (parámetro \texttt{kernel='gaussian'} en \texttt{KernelDensity}):
\[
\hat{p}(\mathbf{x}|y) = \frac{1}{n_y} \sum_{i=1}^{n_y} K_h(\mathbf{x} - \mathbf{x}_i^{(y)})
\]

Donde $K_h$ es el kernel Gaussiano con ancho de banda (bandwidth) $h$:
\[
K_h(\mathbf{u}) = \frac{1}{(2\pi h^2)^{d/2}} \exp\left(-\frac{\|\mathbf{u}\|^2}{2h^2}\right)
\]

El parámetro $h$ controla el suavizado: valores pequeños capturan detalles locales pero son sensibles al ruido, valores grandes producen estimaciones más suaves pero pueden perder estructura. Mediante validación cruzada en el conjunto de entrenamiento, probamos $h \in \{0.1, 0.5, 1.0, 1.5, 2.0\}$ y seleccionamos el valor óptimo $h = 0.1$.

\textbf{Análisis:} Parzen windows con kernel Gaussiano logra clasificación perfecta con $h=0.1$ óptimo. A diferencia de histogramas, Parzen produce estimaciones continuas que funcionan bien incluso con datos relativamente escasos. El suavizado del kernel Gaussiano compensa la falta de muestras, evitando las discontinuidades artificiales de los bins. En validación cruzada, obtiene el mejor F1-macro (0.8648), superior incluso a Naive Bayes (0.8505), confirmando que el bandwidth optimizado generaliza bien.

\subsubsection{k-NN Density Bayes}
\textbf{Test:} Accuracy = 0.4762, F1-macro = 0.5714

\textbf{Fundamento teórico:} Estimador de densidad basado en la distancia al k-ésimo vecino más cercano:
\[
\hat{p}(\mathbf{x}|y) = \frac{k}{n_y \cdot V_k(\mathbf{x})}
\]

Donde $V_k(\mathbf{x})$ es el volumen de la esfera que contiene los $k$ vecinos más cercanos de $\mathbf{x}$ en la clase $y$. Este método adapta el volumen localmente: en regiones densas usa volúmenes pequeños, en regiones dispersas usa volúmenes grandes.

\textbf{Análisis:} Rendimiento pobre en test con k=11 óptimo. Con k=11 en un dataset de 80 muestras de entrenamiento, los 11 vecinos más cercanos diluyen la señal local, resultando en estimación de densidad por volumen local inestable. Clases con distribuciones espaciales dispersas sufren especialmente. Comparado con Parzen, que fija el ancho de banda globalmente, la estrategia de volumen adaptativo local de k-NN Density resulta inferior en este caso.

\subsubsection{k-NN Rule}
\textbf{Test:} Accuracy = 1.0, F1-macro = 1.0

\textbf{Fundamento teórico:} El método más simple de k-NN: asigna la clase mayoritaria entre los $k$ vecinos más cercanos:
\[
\hat{y} = \arg\max_c \sum_{i \in N_k(\mathbf{x})} \mathbb{1}(y_i = c)
\]

Donde $N_k(\mathbf{x})$ son los índices de los $k$ vecinos más cercanos a $\mathbf{x}$. A diferencia de k-NN Density, este método cuenta votos directamente sin estimar densidades.

\textbf{Análisis:} El método clásico de k-NN logra clasificación perfecta con k=1 (vecino más cercano). Esto indica que las clases están bien separadas en el espacio de características y que los ejemplos de test tienen vecinos cercanos de su misma clase en entrenamiento. Con datos bien separados y sin ruido significativo, el vecino más cercano es suficiente y óptimo. Valores mayores de k diluirían innecesariamente la señal. La simplicidad del método (votación directa vs estimación de densidad) resulta ventajosa.

\section{Discusión y Conclusiones}
\label{discussion}

\subsection{Comparación general de enfoques}

Los resultados revelan patrones claros sobre cuándo cada enfoque es apropiado:

\textbf{Modelos paramétricos:}
\begin{itemize}
    \item \textbf{Naive Bayes:} Perfecto (1.0/1.0). La asunción de independencia no penaliza en este dataset. Pocas suposiciones (solo medias y varianzas), robusto con pocas muestras.
    \item \textbf{MLE Full:} Falla con clases minoritarias (0.71/0.46). Más parámetros $\neq$ mejor modelo con datos limitados.
\end{itemize}

\textbf{Modelos no paramétricos:}
\begin{itemize}
    \item \textbf{Parzen:} Perfecto (1.0/1.0) con h=0.1 óptimo. Suavizado continuo supera limitaciones de histogramas.
    \item \textbf{k-NN Rule:} Perfecto (1.0/1.0) con k=1. Simplicidad efectiva cuando datos están separados.
    \item \textbf{Histogram:} Colapso total (0.38/0.08). Maldición dimensionalidad en acción.
    \item \textbf{k-NN Density:} Mediocre (0.48/0.57). Estimación de volumen local inestable.
\end{itemize}

\subsection{Impacto de clases minoritarias}

El dataset Zoo tiene clases con solo 3-4 ejemplos en entrenamiento. Naive Bayes maneja bien clases minoritarias estimando solo 16 parámetros/clase, mientras que MLE Full necesita estimar 136 parámetros/clase (matriz $16 \times 16$) y falla. Parzen y k-NN Rule no estiman parámetros por clase, usan similitud directa y funcionan bien si ejemplos están separados.

\subsection{Métricas: Accuracy vs F1-macro}

En dataset multiclase con distribución desbalanceada (40\% mamíferos vs 4\% anfibios), el F1-macro es esencial. Histogram Bayes tiene 38\% accuracy pero solo 7.9\% F1-macro, revelando que predice solo mamíferos. Los tres modelos perfectos (Naive Bayes, Parzen, k-NN Rule) tienen Accuracy = F1-macro = 1.0, confirmando clasificación genuinamente balanceada.

\subsection{Conclusiones finales}

En el dataset Zoo multiclase con clases minoritarias:

\begin{enumerate}
    \item \textbf{Tres modelos perfectos:} Naive Bayes, Parzen Windows (h=0.1) y k-NN Rule (k=1) logran clasificación perfecta (Accuracy = F1-macro = 1.0)
    \item \textbf{MLE Full falla:} No maneja clases con 3-6 ejemplos (matrices singulares)
    \item \textbf{Histogram colapsa:} Maldición dimensionalidad ($2^{16}$ bins, 101 muestras)
    \item \textbf{k-NN Density mediocre:} Estimación de volumen local inestable
\end{enumerate}

\textbf{Lecciones clave:}
\begin{itemize}
    \item La asunción de independencia de Naive Bayes no siempre penaliza
    \item Modelos complejos necesitan datos: MLE Full requiere $O(d^2)$ muestras/clase
    \item Histogramas sufren curse of dimensionality; Parzen lo mitiga
    \item k-NN simple supera a k-NN density: menos pasos, menos errores
\end{itemize}

Esta práctica demuestra que complejidad $\neq$ mejor rendimiento. Los clasificadores más sencillos (Naive Bayes, k-NN k=1) igualan o superan a métodos sofisticados cuando los datos tienen estructura simple y están bien separados.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning education by demonstrating the practical application of fundamental classification algorithms on real-world datasets. There are no negative societal consequences anticipated from this educational work.

\bibliography{example_paper}
\bibliographystyle{icml2025}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
