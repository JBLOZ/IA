% !TEX program = xelatex
% ¡Recuerda compilar con XeLaTeX o LuaLaTeX!
\documentclass{article}

% --- Cargar nuestro fichero de estilo ---
% Se asume que paper_style.sty está disponible o se usan paquetes estándar.
\usepackage{paper_style}

% --- PAQUETES PARA EL CONTENIDO DEL DOCUMENTO ---
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{wrapfig}

% --- Información del Paper ---
\title{Informe: \\ Práctica 6: Implementación de una canalización de aprendizaje automático con Amazon SageMaker}
\author{
	Jordi Blasco Lozano \\
	\small Infraestructuras y Servicios Cloud \\
	\small Universidad de Alicante
}
\date{\today}

% --- Comienzo del Documento ---
\begin{document}
	
	\maketitle

	\begin{abstract}
	\noindent 
	En esta práctica hemos implementado un pipeline completo de Machine Learning con Amazon SageMaker para detectar anomalías en la columna vertebral de pacientes. Comenzamos explorando el dataset, luego entrenamos un modelo XGBoost, lo desplegamos y evaluamos con diferentes umbrales de clasificación. Finalmente, experimentamos con ajuste automático de hiperparámetros para intentar mejorar el rendimiento. A través de todos estos pasos vimos cómo funcionan los servicios de AWS para ML desde cero hasta tener un modelo en producción.
	\end{abstract}
	\vspace{1cm} 
	
	\tableofcontents

	

	%============================================================
	% SECCIÓN 1: EXPLORACIÓN DE DATOS (LAB 3.2)
	%============================================================
	\section{Exploración de Datos (Lab 3.2)}
	
	\subsection{El Dataset}
	Para esta práctica utilizamos el dataset \texttt{column\_2C\_weka.arff} del repositorio UCI, que contiene información sobre pacientes con columna vertebral. Tiene 310 instancias (100 normales y 210 con anomalías) y 6 atributos biomecánicos:
	
	\begin{itemize}
		\item Incidencia pélvica
		\item Inclinación pélvica
		\item Ángulo de lordosis lumbar
		\item Inclinación del sacro
		\item Radio pélvico
		\item Grado de espondilolistesis
	\end{itemize}

	El objetivo era clasificar a los pacientes en dos categorías: \textbf{Normal} o \textbf{Abnormal}.

	\subsection{Análisis Exploratorio (EDA)}
	
	Lo primero que hicimos fue cargar el dataset y explorar su estructura usando \texttt{shape}, \texttt{describe()} y \texttt{info()}. Luego generamos gráficos de densidad (KDE), histogramas y box plots para entender cómo se distribuían los datos.

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\textwidth]{image copy 12.png}
		\caption{Visualización inicial del conjunto de datos (distribuciones y resumen del EDA).}
	\end{figure}

	

	La característica \texttt{degree\_spondylolisthesis} tenía valores atípicos bastante extremos (alrededor de 400), mientras que el resto de variables estaban más centradas. 

	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{output.png}
			\caption{Vista 1 del EDA}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{output2.png}
			\caption{Vista 2 del EDA}
		\end{subfigure}
		\caption{Gráficos comparativos para identificar distribución y valores atípicos.}
	\end{figure}

	También vimos que había un desbalanceo moderado:  2/3 Abnormal y 1/3 Normal. Esto es importante tenerlo en cuenta más adelante con las métricas.

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\textwidth]{image copy 11.png}
		\caption{Distribución de clases (Normal vs. Abnormal) en el dataset.}
	\end{figure}

	

	\subsection{Tareas de desafios}
	
	Las tareas desafio de este lab tenían como objetivo entender como explorar manualmente todas las características e identificar sus valores atípicos mediante pandas. Para hacer la ultima tarea de desafio, para explorar sobre otros datos de UCI,simplemente me descargue el DataSet que estamos usando en nuestra asignatura de Aprendizaje Automático y le pase las mismas celdas de código. Mi data set era del zoo y tube estos resultados para el mapa de calor.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\textwidth]{output3.png}
		\caption{Mapa de calor (dataset alternativo) usado en la tarea de desafío.}
	\end{figure}



	%============================================================
	% SECCIÓN 2: CODIFICACIÓN DE DATOS CATEGÓRICOS (LAB 3.3)
	%============================================================
	\section{Codificación de Datos Categóricos (Lab 3.3)}
	
	
	En este lab trabajamos con el dataset \texttt{imports-85.csv} que tiene información sobre automóviles. El archivo tiene 205 instancias, 25 atributos y no tenía encabezados, así que hay que definirlos manualmente. El objetivo será aprender a codificar variables categóricas de dos formas diferentes.
	
	Para variables numerales o ordinales (con orden), usamos un diccionario que mapeaba valores a números:

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\textwidth]{image copy 13.png}
		\caption{Ejemplo de codificación ordinal mediante un diccionario (mapeo).}
	\end{figure}

	Para variables nominales (sin orden natural), usamos \texttt{get\_dummies()} que creará nuevas columnas binarias:

	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.29\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image copy 14.png}
			\caption{Antes de \texttt{get\_dummies()}}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.69\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image copy 15.png}
			\caption{Después de \texttt{get\_dummies()}}
		\end{subfigure}
		\caption{Conversión de variables nominales a variables binarias.}
	\end{figure}


	\subsection{Desafío: Añadir Más Variables}
	
	El desafío consistía en codificar variables adicionales: \texttt{fuel-type}, \texttt{body-style} y \texttt{engine-location}. 
	
	pasamos de esto:

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{image copy 16.png}
		\caption{Estado inicial antes de codificar variables adicionales (desafío).}
	\end{figure}

	a esto:

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{image copy 17.png}
		\caption{Resultado tras añadir la codificación de variables adicionales.}
	\end{figure}
		

	

	%============================================================
	% SECCIÓN 3: DIVISIÓN DE DATOS Y ENTRENAMIENTO (LAB 3.4)
	%============================================================
	\section{División de Datos y Entrenamiento (Lab 3.4)}
	
	\subsection{Preparación del Dataset}
	
	Lo primero fue convertir la clase a números: Abnormal = 1, Normal = 0. XGBoost en SageMaker tiene un requisito importante: la columna objetivo debe estar en la primera posición, así que se reordenaron todas las columnas.

	\subsection{División Estratificada}
	
	Dividimos el dataset en tres partes usando \texttt{train\_test\_val} con \texttt{stratify} para mantener la proporción de clases:
	Usamos un 80 por ciento para entrenar, 10 para validación y 10 para test.
	
	Usar \texttt{stratify} asegura que cada conjunto tenga la misma proporción de Normal/Abnormal.
	

	\subsection{Entrenamiento con SageMaker}
	
	Exportamos los CSV sin encabezados ni índices y los cargamos a S3. Luego configuramos XGBoost con los siguientes hiperparámetros:

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.85\textwidth]{image copy 18.png}
		\caption{Hiperparámetros utilizados para el entrenamiento de XGBoost en SageMaker.}
	\end{figure}

	SageMaker se encargó de entrenar el modelo en la nube.



	%============================================================
	% SECCIÓN 4: IMPLEMENTACIÓN Y PREDICCIONES (LAB 3.5)
	%============================================================
	\section{Implementación y Predicciones (Lab 3.5)}
	
	\subsection{Deploy del Modelo}
	
	Una vez entrenado, desplegamos el modelo como un \textit{endpoint} en tiempo real con SageMaker. Esto significa que el artefacto entrenado se levanta en una instancia (por ejemplo, \texttt{ml.m4.xlarge}) y queda expuesto como un servicio que acepta peticiones y devuelve predicciones al momento. Para inferencia, enviamos las características en formato CSV (sin la columna objetivo), y el modelo devuelve una \textbf{probabilidad} (por ejemplo, \texttt{0.9966}) de pertenecer a la clase \textit{Abnormal}. A partir de esa probabilidad, aplicamos un \textbf{umbral} para convertirla a una decisión binaria (0/1), y vimos que variar el umbral cambia el equilibrio entre falsos positivos y falsos negativos.
	\subsection{Transformación por lotes}

	Sin embargo, hacer peticiones fila a fila no es eficiente cuando queremos evaluar un conjunto completo. Por eso, siguiendo el Paso 4 del laboratorio, usamos una \textbf{transformación por lotes (Batch Transform)}: subimos a S3 un CSV con todas las filas de test (solo features), lanzamos un \textit{job} de transformación que procesa el fichero en bloque, y SageMaker escribe automáticamente las probabilidades de salida en otra ruta de S3. Finalmente descargamos ese resultado y aplicamos la misma conversión binaria por umbral para poder comparar predicción vs. etiqueta real y calcular métricas. Esta alternativa evita mantener un endpoint activo y suele ser más adecuada para evaluación offline.
	\subsection{Tarea desafio}
	En la tarea de desafío se nos pedía ir cambiando el valor del umbral de decisión. Al bajarlo, el modelo clasifica como \textit{Abnormal} con más facilidad (aumenta la sensibilidad, pero también los falsos positivos). Al subirlo, se vuelve más conservador (reduce falsos positivos, pero aumenta el riesgo de falsos negativos). En las siguientes capturas se ve cómo cambia la salida al probar distintos valores. En nuestro caso tendríamos el mejor rendimiento en un umbral desde 78 hasta 93. 

	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.49\textwidth}`'
			\centering
			\includegraphics[width=\textwidth]{image copy 3.png}
			\caption{Umbral 0.45}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image copy 4.png}
			\caption{Umbral 0.2}
		\end{subfigure}
		\\
		\vspace{0.5em}
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image copy 5.png}
			\caption{Umbral 0.90}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{image copy 6.png}
			\caption{Umbral 0.95}
		\end{subfigure}
		\caption{Efecto de distintos umbrales en la conversión de probabilidad a clase binaria.}
	\end{figure}

	

	%============================================================
	% SECCIÓN 5: EVALUACIÓN DEL MODELO (LAB 3.6)
	%============================================================
	\section{Evaluación del Modelo (Lab 3.6)}
	
	\subsection{La Matriz de Confusión}
	En este laboratorio evaluamos el modelo entrenado en los pasos anteriores sobre el conjunto de {test}. El modelo devuelve una {probabilidad} de pertenecer a la clase {Abnormal}, y para poder comparar contra las etiquetas reales (0/1) aplicamos un {umbral} $t$ para convertir esa probabilidad en una decisión binaria.

	Con $t = 0.30$ (umbral usado en el notebook base), obtuvimos la siguiente matriz de confusión:

	\begin{table}[H]
		\centering
		\begin{tabular}{lcc}
				oprule
			 & \textbf{Predicho Normal} & \textbf{Predicho Abnormal}\\
			\midrule
				extbf{Real Normal} & 7 & 3\\
				extbf{Real Abnormal} & 2 & 19\\
			\bottomrule
		\end{tabular}
		\caption{Matriz de confusión del modelo con umbral $t=0.30$.}
	\end{table}

	De aquí extraemos: $TN=7$, $FP=3$, $FN=2$ y $TP=19$. En un contexto médico, el error más crítico suele ser el {falso negativo} ($FN$), porque implica {no detectar una anomalía real}. Por otro lado, los {falsos positivos} ($FP$) generan alarmas innecesarias (más pruebas, más tiempo clínico), así que también conviene controlarlos.

	A partir de la matriz calculamos las métricas principales:
	\begin{itemize}
		\item \textbf{Sensibilidad (TPR)} $=\frac{TP}{TP+FN}=90.48\%$.
		\item \textbf{Especificidad (TNR)} $=\frac{TN}{TN+FP}=70.00\%$.
		\item \textbf{Precisión (PPV)} $=\frac{TP}{TP+FP}=86.36\%$.
		\item \textbf{NPV} $=\frac{TN}{TN+FN}=77.78\%$.
		\item \textbf{Exactitud} $=\frac{TP+TN}{TP+TN+FP+FN}=83.87\%$.
	\end{itemize}

	En resumen, con este umbral el modelo detecta la mayoría de casos anormales (alta sensibilidad), pero aún produce algunas falsas alarmas en pacientes normales (FPR=30\%).

	\subsection{Desafío: Comparar Umbrales}
	
	Probamos con umbrales 0.25, 0.30 y 0.75 para ver cómo afectaban a las métricas:

	\vspace{0.5cm}

	Cambiar el umbral no re-entrena el modelo (por tanto no cambia el sobreajuste del entrenamiento), pero sí modifica la política de decisión y desplaza el equilibrio entre falsos positivos y falsos negativos:
	\begin{itemize}
		\item \textbf{Umbral bajo (0.25)}: el clasificador es más \textit{agresivo} marcando \textit{Abnormal}. Normalmente {aumenta la sensibilidad} (baja $FN$) a costa de {aumentar falsos positivos} (sube $FP$ y baja la especificidad).
		\item \textbf{Umbral medio (0.30)}: compromiso intermedio; en nuestro caso dio una sensibilidad del 90.48\% con especificidad del 70.00\%.
		\item \textbf{Umbral alto (0.75)}: el clasificador es más \textit{conservador}. Normalmente {reduce falsos positivos} (sube la especificidad) pero {aumenta el riesgo de falsos negativos}.
	\end{itemize}

	En un escenario de {cribado médico}, suele ser preferible reducir $FN$ (no dejar pasar pacientes con anomalía), aunque eso implique aceptar más $FP$ y luego confirmar con pruebas adicionales.

	\subsection{Curva ROC y AUC}
	
	Generamos la curva ROC (Sensibilidad/TPR vs FPR) para visualizar el rendimiento del modelo a \textbf{todos} los umbrales posibles. El AUC (\textit{Area Under Curve}) resume la ROC en un único número: si es 1.0 es perfecto, si es 0.5 es equivalente a azar, y valores por encima de 0.8 suelen considerarse buenos.

	En nuestros resultados:
	\begin{itemize}
		\item Usando \textbf{probabilidades} (\textit{target\_predicted}), obtuvimos \textbf{AUC $\approx 0.8905$}.
		\item Usando la salida \textbf{binaria} (\textit{target\_predicted\_binary}), el AUC bajó a \textbf{$\approx 0.8024$}.
	\end{itemize}

	Esto es esperable: la ROC está pensada para construirse con \textbf{scores/probabilidades} (que preservan el orden y la confianza de cada predicción). Al binarizar se pierde información y la curva queda menos representativa.

	\begin{figure}[H]
		\centering
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{output4.png}
			\caption{ROC usando probabilidades (AUC $\approx 0.8905$).}
		\end{subfigure}
		\hfill
		\begin{subfigure}[b]{0.49\textwidth}
			\centering
			\includegraphics[width=\textwidth]{output5.png}
			\caption{ROC usando predicción binaria (AUC $\approx 0.8024$).}
		\end{subfigure}
		\caption{Comparativa de curvas ROC: probabilidades vs clases binarizadas.}
	\end{figure}

	

	%============================================================
	% SECCIÓN 6: AJUSTE DE HIPERPARÁMETROS (LAB 3.7)
	%============================================================
	\section{Ajuste de Hiperparámetros (Lab 3.7)}
	En este laboratorio realizamos \textbf{ajuste automático de hiperparámetros} con Amazon SageMaker para intentar mejorar el modelo XGBoost entrenado anteriormente. La idea es ejecutar varios entrenamientos variando parámetros críticos y quedarnos con la configuración que optimiza una métrica objetivo.

	\subsection{Métricas del modelo base}
	Antes de optimizar, calculamos métricas del modelo original para tener una referencia. En este notebook el umbral de decisión se fijó en $t=0.50$ para convertir probabilidades en clases (0/1). Con ese umbral, el modelo base obtuvo:
	\begin{itemize}
		\item \textbf{Accuracy}: 83.87\%.
		\item \textbf{Sensitivity/TPR}: 90.48\%.
		\item \textbf{Specificity/TNR}: 70.00\%.
		\item \textbf{AUC (con salida binaria)}: 0.8024.
	\end{itemize}

	\subsection{Configuración del Tuner}
	Creamos un estimador de XGBoost y definimos la métrica de evaluación que el tuner debe minimizar. En el lab se usa \texttt{error@.40}, que mide la tasa de error binario evaluando como positivo si $p>0.40$ (y negativo en caso contrario). La configuración clave fue:
	\begin{itemize}
		\item \texttt{objective='binary:logistic'}.
		\item \texttt{eval\_metric='error@.40'}.
		\item \texttt{objective\_metric\_name='validation:error'}.
		\item \texttt{objective\_type='Minimize'}.
	\end{itemize}

	Después definimos rangos de búsqueda para los hiperparámetros con mayor impacto en XGBoost:
	\begin{itemize}
		\item \textbf{alpha} (regularización L1): \texttt{[0, 100]}.
		\item \textbf{min\_child\_weight}: \texttt{[1, 5]}.
		\item \textbf{subsample}: \texttt{[0.5, 1]}.
		\item \textbf{eta} (learning rate): \texttt{[0.1, 0.3]}.
		\item \textbf{num\_round}: \texttt{[1, 50]}.
	\end{itemize}

	Finalmente lanzamos el {HyperparameterTuner} con {max\_jobs=10} y {max\_parallel\_jobs=1} (para limitar coste y tiempo), habilitando {early\_stopping\_type='Auto'}.

	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{image copy 10.png}
		\caption{Jobs generados por el ajuste de hiperparámetros (10 entrenamientos).}
	\end{figure}

	\subsection{Ejecución y análisis}
	Tras lanzar el tuner, SageMaker ejecutó los 10 entrenamientos y dejó el proceso en estado {Completed}. Para analizar resultados, cargamos el resumen del tuning en un DataFrame con {HyperparameterTuningJobAnalytics}, que devuelve para cada {job} los hiperparámetros probados y el valor final de la métrica ({FinalObjectiveValue}). Ordenando por esa columna es fácil identificar la mejor combinación.

	A partir del mejor entrenamiento, adjuntamos el {tuner} y creamos el modelo con \linebreak Estimator.attach(best\_training\_job). Después repetimos el flujo de evaluación con {Batch Transform} sobre el conjunto de test y, como en el modelo base, calculamos matriz de confusión y curva ROC.

	\subsection{Resultados obtenidos}
	Este lab muestra que el tuning {no garantiza} una mejora grande en todos los casos. En nuestro escenario influyen varios factores: dataset pequeño, espacio de búsqueda recortado y pocas combinaciones (10). En un entorno real, aumentar {max\_jobs} (30--50+) y ampliar rangos suele ofrecer una mejora más consistente, aunque a costa de más tiempo y coste.

	


\end{document}
