% !TEX program = xelatex
% ¡Recuerda compilar con XeLaTeX o LuaLaTeX!
\documentclass{article}

% --- Cargar nuestro fichero de estilo ---
% Se asume que paper_style.sty está disponible o se usan paquetes estándar.
\usepackage{paper_style}

% --- PAQUETES PARA EL CONTENIDO DEL DOCUMENTO ---
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{float}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{wrapfig}

% --- Información del Paper ---
\title{Informe: \\ Práctica 5: Ingesta, almacenamiento y Serveless}
\author{
	Jordi Blasco Lozano \\
	\small Infraestructuras y Servicios Cloud \\
	\small Universidad de Alicante
}
\date{\today}

% --- Comienzo del Documento ---
\begin{document}
	
	\maketitle

	\begin{abstract}
	\noindent 
	\end{abstract}
	\vspace{1cm} 
	
	\tableofcontents

	\newpage

	\section{Flujo IoT: Ingesta de Telemetría y Almacenamiento Dual}

	
	\subsection{Preparación de Destinos y Configuración IoT}

	\subsubsection{Creación de la Tabla DynamoDB}

\noindent
\begin{minipage}[c]{0.45\textwidth}
	Se ha creado una tabla en DynamoDB llamada \texttt{vigía\_farolas\_estado} para almacenar el último estado reportado por cada farola junto con métricas del consumo y el momento de medir estas métricas. La clave de la tabla será la id de cada farola. Solamente indicando como queremos la clave de partición nuestra tabla será funcional.
\end{minipage}%
\hfill
\begin{minipage}[c]{0.5\textwidth}
	\centering
	\includegraphics[width=\textwidth]{image copy.png}
	\captionof{figure}{Tablas DynamoDB}
\end{minipage}

\vspace{0.5cm}

	\subsubsection{Configuración de AWS IoT Core}
	\begin{itemize}
		\item \textbf{Conexión del dispositivo:} Mediante el asistente de AWS IoT Core registré un dispositivo nuevo. Durante este proceso, se generaron y descargaron los certificados de seguridad \linebreak (\texttt{farolasestado.cert.pem}, la clave privada correspondiente y publica correspondiente). Esto nos servirá para realizar la prueba de conexión y para conectarnos mediante Node-RED posteriormente.
		
		\item \textbf{Prueba de conexión:} Para verificar la conectividad, se ejecutó el script de prueba \texttt{start.ps1} proporcionado por AWS. El script instaló las dependencias necesarias y envió con éxito cinco mensajes de prueba, que fueron validados tanto en la terminal local como en el cliente de pruebas MQTT en el tema sdk/test/js en el mismo panel de conexión de dispositivos IoT.
		
		\begin{figure}[H]
		\centerline{
		\includegraphics[width=\textwidth]{image copy 2.png}}
		\caption{Test start.ps1}
		\end{figure}
		\newpage
		\item \textbf{Cambio en la Política:}
		
\noindent
\begin{minipage}[c]{0.45\textwidth}
	Automáticamente se asocia una política de seguridad, esta política es bastante restrictiva la cambiaremos para que se permitan la conexiones y publicaciones a cualquier tema, yo lo haré en el tema\texttt{"smartcity/consumo/test"}. Usamos `test' porque en ningún momento serán farolas reales por lo que en un supuesto despliegue de la aplicación deberíamos de cambiar el tema. Cabe recalcar que para activar la política debemos de crear una nueva versión seleccionarla como activa y eliminar la versión anterior si no la vamos a volver a usar.
\end{minipage}%
\hfill
\begin{minipage}[c]{0.5\textwidth}
	\centering
	\includegraphics[width=\textwidth]{image copy 14.png}
	\captionof{figure}{Política activa}
\end{minipage}

\vspace{0.5cm}
	\end{itemize}

	\subsection{Creación de la Regla IoT}

	\subsubsection{Consulta SQL}
	Antes de definir la regla, se creó un bucket en S3 para el almacenamiento histórico de la telemetría. La regla de IoT se configuró para procesar los mensajes que llegan al tema \texttt{smartcity/consumo/test}. Para la consulta SQL tuve que cambiar un par de instrucciones ya que no me funcionaba correctamente el \texttt{processed\_time}. Luego veremos lo que use en la función de \texttt{Node-RED} pero por ahora analizaremos la consulta SQL que he utilizado.
	
	\begin{lstlisting}[style=consola, language=bash, caption={Consulta SQL para la regla IoT}]
	SELECT
		consumption_kw,
		timestamp,
		concat(day, '-', month, '-', year, ' ', hour, ':', minute, ':', seconds) AS processed_time
	FROM
		'smartcity/consumo/test'\end{lstlisting}

		
	Como vemos en el SQL hemos quitado el \textbf{home\_id} y hemos cambiado el \textbf{processed\_time}, esto se debe a que AWS no me dejaba utilizar DynamoDBv2 con plantilla y al usar DynamoDB legacy la \texttt{clave\_id} la he mapeado directamente en el apartado correspondiente de DynamoDB, de esta forma ya no me apareciera doble en la tabla de Dynamo (como clave y como atributo). Posteriormente cambie el \texttt{proccesed\_time} porque no hubo forma de procesar el tiempo con la función propuesta. Esta ha sido la forma más sencilla de obtener el tiempo procesado. Podría haberlo procesado al crear el mensaje cambiando la función del Node-RED o después al recibir el mensaje, y he decidido hacerlo después para que las carpetas del S3 pudieran mapear mucho mas facil el tiempo, y así crear las carpetas sin errores.
	\subsubsection{Acción S3 bucket}
	En esta acción también cambiamos un apartado, la ``clave'', en el enunciado se pedía utilizar un almacenamiento en carpetas clasificadas por año por mes por dia, el problema que le vi principalmente fue que en un sistema real podríamos tener diferentes farolas por lo que utilice una ultima clasificación que en la carpeta dias tuviera carpetas por ``farola\_id''. Dentro de esta ultima carpeta ya tenemos cada archivo ordenado por la hora, minuto y segundo usando ``hh:mm:ss-data.json'' en vez de con timestamp (que a mi se me ponia en milisegundos). He usado la siguiente clave con los parámetros obtenidos directamente desde sus variables del mensaje (en vez de desde el timestamp).

	\begin{lstlisting}[style=consola, language=bash, caption={Clave del S3}]
		farolas/year=${year}/month=${month}/day=${day}/${home_id}/${hour}:${minute}:${seconds}-data.json
\end{lstlisting}




	\subsubsection{Accion DynamoDB}
	En esta otra acción se pedía usar DynamoDBv2, despues de probar mil formas cambie a la versión a la de DynamoDB normal en la cual nosotros mismos debemos de vincular la variable clave para usarla como clave de particion de nuestra tabla. Lo hice manualmente porque no sabia como insertar la plantilla de atributos que proporciona el enunciado. Y no conseguí que las claves se vincularan automáticamente, por lo que lo hice manualmente con DynamoDB de la siguiente forma.

		\begin{figure}[H]
		\centerline{
		\includegraphics[width=\textwidth]{image copy 13.png}}
		\caption{Configuración de la acción DynamoDB}
		\end{figure}

		

	\subsection{Simulación de ingesta con Node-RED}

		Node-RED permite conectarse mediante MQTT a nuestro tema de `thing' de IoT para simular el envío de datos de las farolas. El flujo consta de tres nodos:
		\begin{itemize}
			\item \textbf{Inject:} Un disparador manual para iniciar el flujo, cada 10 segundos se envia una señal al siguiente nodo para que mande un mensaje.
			\item \textbf{Function:} Un nodo que construye el mensaje JSON con los datos de la farola (ID, consumo y el tiempo) y establece el tema de destino en \texttt{smartcity/consumo/test}. Este nodo consta de una serie de variables que hemos randomizado para simular la farola. He utilizado 6 variables de más para el tiempo, de forma que tengamos en variables separadas el año, el mes, el dia, la hora, los minutos y los segundos. Consiguiendo que los pasos anteriores hayan sido más sencillos de implementar sabiendo que no tenemos que parsear ni usar funciones diferentes para obtener los datos del tiempo.
			
			\begin{lstlisting}[style=consola, language=bash, caption={Funcion constructora del mensaje}]
const now = new Date();
msg.topic = "smartcity/consumo/test";
msg.payload = {
    home_id: "farola-001",
    consumption_kw: +(Math.random() * 2).toFixed(2),
    timestamp: now.getTime(),
    year: now.getFullYear(),
    month: String(now.getMonth() + 1).padStart(2, "0"),
    day: String(now.getDate()).padStart(2, "0"),
    hour: String(now.getHours()).padStart(2, "0"),
    minute: String(now.getMinutes()).padStart(2, "0"),
    seconds: String(now.getSeconds()).padStart(2, "0"),
};
return msg;\end{lstlisting}
			 
			\item \textbf{MQTT Out:} Este nodo está configurado para publicar el mensaje en el endpoint de AWS IoT (a1paa0c5brn8mp-ats.iot.us-east-1.amazonaws.com) a través del puerto 8883 con TLS. La autenticación se realizó utilizando los mismos certificados de dispositivo que en la prueba de conexión inicial. Usamos TLS el cual contiene todos los certificados necesarios para conecarnos al endpoint que anteriormente hemos introducido.
		\end{itemize}

		Con estos tres nodos configurados tenemos ya el flujo funcional, y en darle a instanciar tendremos el sistema enviando mensajes cada 10 segundos al tema de MQTT de nuestro objeto. Para parar el sistema es tan simple como dehabilitar el nodo del disparador y volver a darle a instanciar.
		\begin{figure}[H]
		\centerline{
		\includegraphics[width=\textwidth]{image copy 4.png}}
		\caption{Flujo Node-RED}
		\end{figure}


\subsubsection{Pruebas}
\begin{itemize}
\item \textbf{Cliente de prueba MQTT:} Para comprobar que cada una de las acciones anteriores de la regla funcionen debemos de comprobarlo lanzando el Node-RED. El primer paso es comprobar que los mensajes son enviados al tema correspondiente, para esto nos vamos al cliente de prueba de MQTT dentro de la pestaña de IoT de AWS y nos subscribimos a \texttt{smartcity/consumo/\#} cuando nos subscribamos nos empezaran a salir mensajes como estos.


	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{image copy 9.png}}
	\caption{Cliente de prueba de MQTT}
	\end{figure}

\item \textbf{S3:} 
Respecto al guardado de mensajes historicos debemos de comprobarlo dentro del S3 de la practica, nos dirigimos a la sucesiones de carpetas que tenemos configurada y dentro de la ultima vemos como tenemos estos datos. Los datos de dentro de cada archivo tambien corresponden con el .json definido en el SQL, ej: 

\begin{lstlisting}[style=consola, language=bash]
{"consumption_kw":0.3,"timestamp":1763374077358,"processed_time":"17-11-2025 11:07:57"}
\end{lstlisting}

	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{image copy 10.png}}
	\caption{S3 bucket}
	\end{figure}

	
\item \textbf{DynamoDB}
Finalemente debemos de comprobar la tabla de DynamoDB, dentro de \linebreak \texttt{vigia\_farolas\_estado}, y en explorar elementos de la tabla vemos como nuestra tabla contiene una \texttt{farola001} con el json correspondiente. Este json se va actualizando cada 10 segundos y muestra siempre el estado actual de la farola. Si tuviéramos mas farolas se mostraría una tabla con el estado actual de cada una de las farolas.

	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{image copy 11.png}}
	\caption{Tabla DynamoDB elementos}
	\end{figure}

\end{itemize}



\newpage


\section{Flujo de Contexto: Web Scraping con Lambda Serverless (RA2)}
	
\subsection{Creación de la Función Lambda}

Para crear la función Lambda debemos de ingresar al servio Lambda y crear una función de python desde cero con los permisos de Lab-Role para poder ejecutar correctamente el proceso y realizar el PutObject al S3. Una vez dentro se nos abrirá un editor de código, que en si es un fork web de visual Studio Code. Debemos de pasar la variable por entorno, para esto dentro del panel de configuración ingresamos a variables de entorno y copiamos el nombre del bucket con el nombre de la variable del código proporcionado.

	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{image copy 15.png}}
	\caption{Variable de entorno}
	\end{figure}

Cuando tengamos la variable de entorno copiamos el código que simulará nuestra ingesta de datos de contexto aletorizando la tarifa de luz del día \texttt{tarifakwh}. Tras copiar el código ya podemos desplegarlo.

\subsection{Orquestación Serverless (EventBridge)}

\noindent
\begin{minipage}[c]{0.55\textwidth}
	Al igual que hemos hecho en el apartado anterior con la ingesta de telemetría mediante la simulación por Node-RED, en este apartado también tenemos que simular una entrada programada cada cierto tiempo. En este caso mandaremos una señal a nuestra función Lambda cada 1 minuto. Esto lo haremos mediante el servicio de Amazon EventBridge, este nos permitirá crear un evento programado al destino de AWS que nosotros queramos. Lo configuraremos de la siguiente forma eligiendo nuestra función Lambda y poniendo una programación que se repita cada minuto.
	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{image copy 17.png}}
	\caption{Frecuencia Event Bridge}
	\end{figure}

\end{minipage}%
\hfill
\begin{minipage}[c]{0.4\textwidth}
	\centering
	\includegraphics[width=\textwidth]{image copy 16.png}
	\captionof{figure}{Destino Event Bridge}
\end{minipage}

\vspace{0.5cm}


Si nos fijamos en el código de nuestra función, el guardado de datos se sobrescribira al poner como nombre DD-MM-YYYY-data.json. Pero podremos comprobar que se sobrescriba correctamente comprobando el contenido del archivo del S3 cada minuto. Nuestros archivos \texttt{-data.json} tendrán la estructura de este ejemplo.

\begin{lstlisting}[style=consola, language=bash]
{"timestampscraper": "2025-11-17T18:20:11.307175", "tarifakwh": 0.1833, "ciudad": "Neo-Tech"}
\end{lstlisting}

Nuestra carpeta dentro del bucket dentra esta otra forma.
	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{image copy 18.png}}
	\caption{S3 tarifas}
	\end{figure}


Finalmente en los registros de cloudWatch podemos ver las invocaciones a la función mas recientes.
	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{image copy 19.png}}
	\caption{S3 tarifas}
	\end{figure}

	\newpage


	\section{Flujo Masivo: Transformación y Análisis Serverless}

	

\noindent
\begin{minipage}[c]{0.45\textwidth}
	\subsection{Carga y Catalogación (RA 1)}
	Para cargar el dataset historico de datos de movilidad hice uso de una carpeta nueva en nuestro S3 de la practica, la llame \texttt{/historicomovilidad} e introduje el \texttt{.csv} proporcionado en la practica, utilice el de 10 filas para evitar consumos. Antes de subirlo lo abrí en visual studio code y me di cuenta de que me saltaba el linter porque al haber eliminado la categoria repetida \texttt{Airport\_fee} no me había fijado que tambíen debía eliminar las comas de esta columna.
	\subsubsection{Carga Masiva}
	Con el \texttt{.csv} listo y la carpeta creada cargué el archivo.
\end{minipage}%
\hfill
\begin{minipage}[c]{0.5\textwidth}
	\centering
	\includegraphics[width=\textwidth]{image copy 34.png}
	\captionof{figure}{csv historico movilidad}
\end{minipage}

\vspace{0.5cm}
	

	\subsubsection{AWS Glue Crawler}
	Mediante AWS Glue Crawler, automatizamos el proceso de descubrimiento y catalogación de los datos almacenados en el S3. El Crawler escanea los archivos en la carpeta, infiere su esquema (nombres de columnas, tipos de datos y particiones) y crea las Glue Tables dentro de nuestra base de datos.

	Es importante destacar que las tablas creadas en el Data Catalog no contienen los datos en sí, sino metadatos: apuntan a la ubicación física de los archivos en S3 y definen su estructura. Esto actúa como una capa de abstracción que permite a servicios como Amazon Athena consultar los archivos CSV (u otros formatos) directamente usando SQL estándar, como si se tratara de una base de datos relacional tradicional, sin necesidad de mover o cargar los datos previamente. El crawler lo configuramos de la siguiente forma: 

\noindent
\begin{minipage}[c]{0.35\textwidth}
	\begin{itemize}
	\item \textbf{Creación del Crawler:} en este apartado debemos de definir el nombre del crawler (el mio vigia-crawler) y dentro de su panel de configuración debemos de añadir nuestra fuente de datos (la dirección S3 de nuestra carpeta \texttt{/historicomovilidad} que creamos anteriormente). Posteriormente asignamos nuestro IAM role (el LabRole) y finalmente creamos una base de datos vacia para que el Crawler genere las tablas dentro de ella. Al acabar estos pasos debemos de tener un Crawler como este:
	\end{itemize}
\end{minipage}%
\hfill
\begin{minipage}[c]{0.57\textwidth}

	\centering
	\includegraphics[width=\textwidth]{image copy 30.png}
	\captionof{figure}{Crawler}
\end{minipage}

\vspace{0.5cm}
\begin{itemize}
	\item \textbf{Ejecución del Crawler:} tras terminar la configuración del Crawler debemos de ejecutarlo. Durante este proceso de ejecución el crawler lo podemos observar en los logs de ejecución: el crawler \texttt{vigia-movilidad-crawler} inicia el proceso de clasificación de los datos. Una vez analizada la estructura de los archivos en S3, escribe los resultados en la base de datos \texttt{vigia\_datalake\_db}, creando exitosamente la tabla \texttt{historicomovilidad}. Finalmente, actualiza el Data Catalog (indicando \texttt{ADD: 1} en los logs) y vuelve al estado \texttt{READY}, dejando los datos listos para ser consultados. Para comprobar si ha el crawler ha conseguido analizar correctamente la escructura accedemos a las tablas de nuestro Data Catalog y dentro de la tabla \texttt{historicomovilidad} deberá de aparecernos este esquema:
	
	\begin{figure}[H]
	\centerline{
	\includegraphics[width=0.9\textwidth]{image copy 35.png}}
	\caption{Table Schema}
	\end{figure}

	\item \textbf{Consulta SQL:} Al tener la tabla ya creada, podemos consultar el archivo csv directamente con SQL mediante el servicio de Athena que proporciona amazon. Para las guardar las consultas lo optimo será crear un S3 nuevo que en mi caso he llamado \texttt{resultados-p5-backet} que guardará tanto las consultas que hagamos con esta tabla (de datos en bruto) como las que hagamos con la tabla (analítica). He obtenido estos resultados al hacer la consulta:

	\begin{figure}[H]
	\centerline{
	\includegraphics[width=0.9\textwidth]{image copy 36.png}}
	\caption{Table Schema}
	\end{figure}

	
	\end{itemize}


	\subsection{Optimización del Data Lake (AWS Glue ETL) (RA 2)}
	En este paso, implementaremos un proceso ETL (Extract, Transform, Load) utilizando un Job de AWS Glue. El objetivo principal es transformar los datos crudos que tenemos en formato CSV a un formato optimizado para analítica (Parquet) y particionarlos adecuadamente. Esto nos servirá para mejorar drásticamente el rendimiento de las consultas en Athena y reducir los costos, ya que Parquet es un formato columnar comprimido que permite leer solo las columnas necesarias, y el particionado permite escanear solo los datos relevantes para una consulta (por ejemplo, filtrar solo por un año y mes específicos).

	El Job realizará las siguientes acciones clave:
	\begin{itemize}
		\item \textbf{Extracción:} Leerá los datos de la tabla original catalogada en el Data Lake.
		\item \textbf{Transformación:} Convertirá los tipos de datos (casteo de strings a double/timestamp), limpiará el formato de fecha y seleccionará las columnas críticas para el análisis.
		\item \textbf{Carga:} Escribirá los datos procesados en una nueva carpeta \texttt{/analitica} del S3 en formato Parquet, particionados por año y mes.
	\end{itemize}

	\subsubsection{Creación y Propiedades del Job}
	Para crear nuestro job nos dirigimos a AWS glue, a ETL Jobs y creamos uno nuevo. Este paso es sencillo ya que al tener ya el código python tan solo tendremos que pegarlo en la pestaña de scripts eligiendo antes la version de python 3, Spark 3.5, amazon glue 5.0 y el LabRole.


	\subsubsection{Configuración del Script de Transformación}
	Al acabar configurar el entorno Serverless debemos de pegar el archivo python cambiando los valores de nuestra tabla y base de datos a \texttt{historicomovilidad} y \texttt{vigia\_datalake\_db} respectivamente. También debemos de cambiar el nombre de la variable S3 para que apunte a nuestra carpeta de analisis (que debemos de crear) dentro de nuestro S3. En esta carpeta es donde se realizarán las particiones y se guardaran los archivos Parquet que genere nuestro job. \\

	Sabremos que nuestro job ha concluido con exito cuando en runs nos salga lo siguiente.
	\begin{figure}[H]
	\centerline{
	\includegraphics[width=0.95\textwidth]{image copy 37.png}}
	\caption{Job run}
	\end{figure}


	\noindent
	\begin{minipage}[c]{0.42\textwidth}
		
	Dentro de la carpeta \texttt{/analisis} de nuestro S3 debemos de comprobar si realmente ha escrito los archivos \texttt{Parquet} correspondientes.


	Al inspeccionar la salida en S3, observamos que se ha generado una estructura de carpetas jerárquica correspondiente a las claves de partición definidas en el script (\texttt{year} y \texttt{month}). En este caso específico, solo vemos la ruta \texttt{year=2023/month=6/} conteniendo un único archivo Parquet. Esto se debe a que nuestro dataset de prueba (las 10 filas del CSV) es minúsculo y contiene exclusivamente registros del 30 de junio de 2023.

	Es importante aclarar que en un entorno de producción con muchos datos, es normal y deseable encontrar múltiples archivos Parquet dentro de una misma carpeta de mes. Esto ocurre porque Glue procesa los datos en paralelo; cada proceso "worker" escribe su propia parte de los datos simultáneamente para maximizar el rendimiento. Aunque Athena leerá automáticamente todos los archivos que encuentre dentro de la carpeta de la partición como si fueran una única tabla.
		

	\end{minipage}%
	\hfill
	\begin{minipage}[c]{0.55\textwidth}
		\centering
		\includegraphics[width=\textwidth]{image copy 38.png}
		\captionof{figure}{Archivo Parquet}
	\end{minipage}



	\subsection{Creación del Crawler PROCESADO}
	Para finalizar este flujo y poder explotar estos datos optimizados, es necesario catalogar esta nueva estructura en el Glue Data Catalog. Esto se realiza creando y ejecutando un segundo Crawler que apunte específicamente a la carpeta \texttt{/analitica}. Una vez completado, tendremos una nueva tabla \texttt{analitica\_movilidadanalitica} en la base de datos \texttt{vigia\_datalake\_db}. En esta nueva tabla podemos ver como el crawler ha analizado la carpeta \texttt{/analitica} y a parte de extraer las columnas con los nuevos tipos de datos, ha obtenido las particiones de año y mes.

	\begin{figure}[H]
	\centerline{
	\includegraphics[width=0.8\textwidth]{image copy 32.png}}
	\caption{Schema analitica\_movilidadanalitica}
	\end{figure}


	\subsection{Consultas con Athena}
	Como se ha explicado en apartados anteriores, Amazon Athena permite realizar consultas SQL directamente sobre las tablas definidas en el catálogo de AWS Glue. Con el objetivo de comparar el rendimiento entre la tabla de datos brutos en formato CSV y la tabla de datos analíticos optimizados, se han diseñado dos consultas filtrando por el mismo periodo temporal (2023-06).

	En la Consulta A se utiliza la tabla histórica sin procesar (historico\_movilidad), cuyo esquema incluye, entre otras, la columna de fecha y hora tpep\_pickup\_datetime sobre la que se aplica el filtro.

	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{primera.jpeg}}
	\caption{Consulta A}
	\end{figure}


	Al no existir particiones por año y mes en esta tabla, Athena debe escanear el CSV completo y aplicar el filtro fila a fila sobre tpep\_pickup\_datetime, por lo que el valor de \textbf{Datos analizados} es elevado.

	En la Consulta B se trabaja sobre la tabla procesada analitica\_movilidad, donde el dataset se ha transformado a formato Parquet y se ha particionado por las columnas year y month.

	\begin{figure}[H]
	\centerline{
	\includegraphics[width=\textwidth]{segunda.jpeg}}
	\caption{Consulta B}
	\end{figure}

	Al filtrar directamente por las columnas de partición, Athena solo lee los archivos Parquet ubicados en la ruta year=2023/month=6/ del bucket S3, reduciendo de forma muy significativa los datos escaneados y demostrando la mejora de rendimiento y coste que aporta la optimización serverless del Data Lake.
	
	\section{Conclusiones:}

	Tras acabar la práctica, me quedo con varias ideas que considero bastante representativas de lo que realmente implica montar una arquitectura de datos serverless en AWS para un caso como el de la Smart City. Todo el proceso, desde la ingesta con Node-RED y la integración con AWS IoT Core, hasta la automatización con Lambda y EventBridge, ha permitido ver de primera mano cómo se pueden orquestar servicios gestionados para cubrir tanto las consultas históricas (S3) como el estado en tiempo real (DynamoDB).
\\\\
	Con la parte de Glue y Athena me ha quedado claro que la clave está en cómo estructuramos la información. Tener los datos en CSV es funcional para el inicio, pero en cuanto necesitas rendimiento y consultas eficientes, el particionado y el formato Parquet marcan la diferencia. La cantidad de datos escaneados y el tiempo de ejecución cambian radicalmente según el tipo de tabla y el tratamiento que se les haya dado en el ETL.
\\\\
	También he aprendido lo útil que es automatizar flujos con EventBridge y Lambda, sobre todo cuando hay entradas de contexto que no siguen el mismo patrón que los datos IoT. Integrar scraping periódico y guardar los resultados en el Data Lake resulta sencillo, de esta forma se mantiene el sistema desacoplado para cuando queramos incluir más datos a parte de la tarifa.
\\\\
	Otra parte interesante ha sido comprobar cómo Glue Crawler, detectando automáticamente los esquemas y particiones, facilita muchísimo la catalogación de datos para crear facilmente nuevas tablas. 
\\\\
	En definitiva, el trabajo que hemos realizado muestra de forma práctica que el éxito en arquitecturas de datos modernas depende de manejar bien el ciclo de vida del dato, automatizar todo lo posible y, sobre todo, diseñar pensando en el rendimiento desde el principio. Esto no solo aplica para este caso, sino para cualquier entorno donde se trabaje con datos y sea necesario combinarlos, analizarlos y servirlos de forma flexible y eficiente.


	
\end{document}
